{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TP3_INF8215-3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5fHWJhwOXGzh"
      },
      "source": [
        "# INF8215 - Intelligence artif.: méthodes et algorithmes \n",
        "## Automne 2019 - TP3 - Apprentissage machine \n",
        "### Membres de l'équipe\n",
        "    - Kacem Khaled\n",
        "    - Oumayma Messoussi\n",
        "    - Semah Aissaoui\n",
        "\n",
        "Nom d'équipe sur Kaggle: Procrastinators\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TmGvqtSVgfXi"
      },
      "source": [
        "## Directives de remise\n",
        "Le travail sera réalisé avec la  même équipe que pour les TPs précédents. Vous remettrez ce fichier nommé TP3\\_NomDuMembre1\\_NomDuMembre2\\_NomDuMembre3.ipynb et vos fichiers de soumission csv dans la boîte de remise sur moodle. \n",
        "\n",
        "Tout devra être remis avant le **13 décembre à 23h55**. Tout travail en retard sera pénalisé d’une valeur de 10\\% par jour de retard.\n",
        "\n",
        "## Barème\n",
        "Partie 1: 9 points\n",
        "\n",
        "Partie 2: 9 points\n",
        "\n",
        "Partie 3: 2 points\n",
        "\n",
        "Bonus : 2 points\n",
        "\n",
        "Pour un total de 22 points possibles sur 20 points.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "P1et8f3nXGzk"
      },
      "source": [
        "## Apprentissage machine\n",
        "L'apprentissage machine est un domaine de plus en plus exploré et exploité. Pouvoir faire des prédictions en se basant sur des données ouvre la porte à plein de possibilités pour faciliter la vie des gens. Par exemple, un modèle pourrait prédire si un patient souffre ou non d'une maladie en se basant sur des mesures de santé.\n",
        "\n",
        "### But\n",
        "Le but de ce TP est de vous donner un aperçu du déroulement général d'un projet de machine learning tout en vous familiarisant avec des librairies python adaptées.\n",
        "\n",
        "Dans la première partie, vous implémenterez un algorithme de classification multiclasse appelé **softmax regression** à l'aide uniquement de la bibliothèque **numpy** et l'intégrerez à la bibliothèque **scikit-learn**.\n",
        "\n",
        "Dans la deuxième partie, vous prendrez connaissance du **dataset** utilisé pour ce projet. Et vous serez amenés à effectuer le **preprocessing** de ces données pour qu'elles soient utilisables dans les algorithmes de machine learning classiques. Vous utiliserez les bibliothèques **pandas** et **scikit-learn**.\n",
        "\n",
        "Enfin, dans la troisième partie, vous comparerez l'efficacité du modèle que vous avez implémenté avec d'autres modèles déjà implémentés dans **sklearn**. Puis vous tenterez d'améliorer les performances de l'algorithme sélectionné.\n",
        "\n",
        "La dernière partie du TP est de soumettre vos prédictions sur **Kaggle**. Vous devez créer un compte et formez une équipe sur Kaggle: https://www.kaggle.com/. La compétition pour le TP se trouve à l'adresse suivante: https://www.kaggle.com/t/7e5ae1ae92d14e06b2560c9ac5602bf6.\n",
        "\n",
        "### Installation\n",
        "\n",
        "Pour installer **pandas** et **scikit-learn** le plus simple est de télécharger et d'installer **Anaconda**. Cet environnement python regroupe les packages les plus utilisés pour le calcul scientifique et la science des données. Vous pouvez aussi installer directement les packages avec pip.\n",
        "\n",
        "Assurez-vous d'avoir au moins la version **20.0** de **scikit-learn**.\n",
        "\n",
        "Vous trouverez la distribution d'anaconda ici : https://www.anaconda.com/download/ .\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LYyOL7QAXGzm"
      },
      "source": [
        "## Bonus 1: Compétition (2 points)\n",
        "\n",
        "Lorsque vous soumettez vos résultats sur Kaggle, vous obtiendrez votre performance en terme de **Mean F-Score**. Votre score sera affiché sur le leaderboard publique de Kaggle.  Le top 10 sur le leaderboard privée recevront un bonus de 2pts. Vous pouvez soumettre plusieurs fois et choisir 2 soumissions pour être pris en compte pour le classement final. Attention de ne pas overfit sur les données de tests du classement publique."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "517JwzlPXGzp"
      },
      "source": [
        "# 1. Softmax Regression (12 points)\n",
        "\n",
        "Dans cette partie vous implémenterez **softmax regression** la généralisation de **logistic regression** qui permet d'effectuer de la classification pour un nombre de classe supérieur à 2.\n",
        "\n",
        "**Pour cet exercice, la contrainte est d'utiliser uniquement la bibliothèque numpy**\n",
        "\n",
        "## Encapsulation avec sklearn\n",
        "\n",
        "La classe **SoftmaxClassifier** hérite des classes **BaseEstimator** et **ClassifierMixin** de **scikit-learn** ce qui nous permettra d'utiliser facilement avec notre classifier les outils fournis par scikit-learn dans la suite du TP.\n",
        "\n",
        "Pour la compatibilité, le classifier doit implémenter obligatoirement les méthodes:\n",
        "\n",
        "* **fit**: responsable de l'entraînement du modèle\n",
        "* **predict_proba**: permet de prédire la probabilité de chaque classe pour chaque exemple du dataset fourni.\n",
        "* **predict**: permet de prédire la classe pour chaque exemple du dataset fourni.\n",
        "* **score**: permet de quantifier l'écart entre les classes prédites et les classes réelles pour le dataset fourni\n",
        "* **fit_predict**: permet de faire un fit et retourne les prédictions faites avec cet ensemble.\n",
        "\n",
        "\n",
        "## Train/Test set:\n",
        "\n",
        "Quand on veut tester les performances de l'apprentissage d'un algorithme de machine learning, on **ne le teste pas sur les données utilisées pour l'apprentissage**.\n",
        "\n",
        "En effet, ce qui nous intéresse c'est que notre algorithme soit **capable de généraliser** ses prédictions à des données qu'il n'a **jamais vu**.\n",
        "\n",
        "Pour illustrer, si on teste un algorithme sur les données d'entrainement, on teste sa capacité à **apprendre par coeur** le dataset et non à **généraliser**.\n",
        "\n",
        "Par conséquent, quand on reçoit un nouveau dataset, la première chose à faire et de le **diviser en deux parties**: un ensemble d'**entraînement** (**70-80%** du dataset) et un ensemble de **test**(**20-30%** du dataset).\n",
        "\n",
        "Tous les algorithmes de **traitement des données** et d'apprentissage devront être appris uniquement sur l'ensemble d'entraînement et appliqués ensuite sur l'ensemble de test.\n",
        "\n",
        "Cela garantit l'absence de connaissances préalables de l'ensemble de test lors de l'entrainement.\n",
        "\n",
        "## Gradient descent\n",
        "\n",
        "La descente de gradient est un algorithme qui permet trouver la solution optimale d'un certains nombre de problèmes. Le principe est le suivant: on définit une **fonction de coût J**  qui caractérise le problème.\n",
        "Cette fonction dépend d'un ensemble de paramètres $\\theta$. La descente de gradient cherche à **minimiser** la fonction de coût en **modifiant itérativement** les paramètres.\n",
        "\n",
        "### Gradient\n",
        "\n",
        "Le gradient de la fonction de coût pour un $\\theta$ donné, correspond à la direction dans laquelle il faut modifier $\\theta$ pour augmenter la valeur de la fonction de coût. Cela correspond donc à la dérivée de la fonction de coût.\n",
        "La fonction de coût est minimale quand le gradient est nul.\n",
        "\n",
        "Concrètement, on initialize $\\theta$ aléatoirement, et on effectue à chaque itération un pas pour réduire la fonction de coût jusqu'à convergence de l'algorithme à un minimum.\n",
        "\n",
        "### Learning rate\n",
        "\n",
        "Le taux d'apprentissage correspond à la taille du pas que l'on va effectuer dans la direction du gradient.\n",
        "Plus il est grand, plus la convergence est rapide mais il y a un risque que l'algorithme diverge.\n",
        "\n",
        "Plus il est petit, plus la convergence est lente.\n",
        "\n",
        "### Batch gradient descent\n",
        "\n",
        "Il existe plusieurs algorithmes de descente de gradient. Nous utiliserons Batch gradient descent.\n",
        "\n",
        "Dans cet algorithme, avant de mettre à jour $\\theta$, on calcule les gradients sur l'ensemble des exemples d'entraînement.\n",
        "\n",
        "### Epoch\n",
        "\n",
        "Il s'agit d'un pas de la descente de gradient, soit une mise à jour de gradient.\n",
        "\n",
        "### Bias/Variance tradeoff\n",
        "\n",
        "Lorsqu'on entraine un algorithme de machine learning on cherche un équilibre entre **biais** et **variance**.\n",
        "\n",
        "Un modèle avec un **biais fort**, est un modèle qui est **trop simple** pour la structure donnée considérée (modèle linéaire pour données quadratiques), cela limite la capacité du modèle à généraliser. On appelle aussi le biais **underfitting**.\n",
        "\n",
        "Un modèle avec une **variance élevée** signifie qu'il est sensible aux petites variations dans les données d'entrainement, cela correspond à l'**overfitting**, c'est-à-dire que le modèle est trop proche de la structure de l'ensemble d'entrainement ce qui **limite sa capacité à généraliser**.\n",
        "\n",
        "Un modèle avec un **biais important** aura une **mauvaise performance** sur l'ensemble d'**entraînement**.\n",
        "Un modèle avec une **variance importante** aura une performance bien **moins bonne** sur l'ensemble de **test** que sur l'ensemble d'**entrainement**.\n",
        "\n",
        "Le classifier **SoftmaxRegression** contient les champs suivants:\n",
        "\n",
        "- `lr` : le rythme d'apprentissage pour la mise à jour des points lors de la descente du gradient;\n",
        "- `n_epochs`: le nombre d'itérations;\n",
        "- `eps`: le seuil pour garder les probabilités dans l'intervalle [self.eps;1.-self.eps];\n",
        "- `threshold`: seuil d'arrèt pour le early stopping;\n",
        "- `early_stopping`: active le early stopping\n",
        "- `nb_features`: le nombre d'attributs pour chaque instance\n",
        "- `nb_classes`: le nombre de classes possible pour le dataset\n",
        "- `theta_`:  la matrice de poids"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UhACgVBXAcUW",
        "colab": {}
      },
      "source": [
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class SoftmaxClassifier(BaseEstimator, ClassifierMixin):\n",
        "\n",
        "    def __init__(self, lr=0.1, alpha=100, n_epochs=1000, eps=1.0e-5, threshold=1.0e-10, early_stopping=True):\n",
        "\n",
        "        self.lr = lr\n",
        "        self.alpha = alpha\n",
        "        self.n_epochs = n_epochs\n",
        "        self.eps = eps\n",
        "        self.threshold = threshold\n",
        "        self.early_stopping = early_stopping\n",
        "\n",
        "    \"\"\"\n",
        "        In:\n",
        "        X : l'ensemble d'exemple de taille nb_example x nb_features\n",
        "        y : l'ensemble d'étiquette de taille nb_example x 1\n",
        "\n",
        "        Principe:\n",
        "        Initialiser la matrice de poids\n",
        "        Ajouter une colonne de bias à X\n",
        "        Pour chaque epoch\n",
        "            calculer les probabilités\n",
        "            calculer le log loss\n",
        "            calculer le gradient\n",
        "            mettre à jouer les poids\n",
        "            sauvegarder le loss\n",
        "            tester pour early stopping\n",
        "\n",
        "        Out:\n",
        "        self, in sklearn the fit method returns the object itself\n",
        "    \"\"\"\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "\n",
        "        prev_loss = np.inf\n",
        "        self.losses_ = []\n",
        "\n",
        "        self.nb_feature = X.shape[1]\n",
        "        self.nb_classes = len(np.unique(y))\n",
        "        #self.nb_example = X.shape[0]\n",
        "\n",
        "        X_bias = np.concatenate((np.ones((X.shape[0], 1)), X), axis=1)\n",
        "        self.theta_ = np.random.rand(self.nb_feature + 1, self.nb_classes)\n",
        "\n",
        "        for epoch in range(self.n_epochs):\n",
        "            #logits = np.dot(X_bias, self.theta_)\n",
        "            #probabilities = self._softmax(logits)\n",
        "            \n",
        "            probabilities = self.predict_proba(X)\n",
        "            \n",
        "            loss = self._cost_function(probabilities, y)\n",
        "            self.theta_ = self.theta_ - self.lr * self._get_gradient(X_bias, y, probabilities)\n",
        "\n",
        "            self.losses_.append(loss)\n",
        "            if self.early_stopping:\n",
        "                if (epoch > 0) and (abs(self.losses_[epoch-1] - self.losses_[epoch]) < self.threshold):\n",
        "                    break\n",
        "        return self\n",
        "\n",
        "    def predict_proba(self, X, y=None):\n",
        "        try:\n",
        "            getattr(self, \"theta_\")\n",
        "        except AttributeError:\n",
        "            raise RuntimeError(\"You must train classifer before predicting data!\")\n",
        "\n",
        "        X_bias = np.concatenate((np.ones((X.shape[0], 1)), X), axis=1)\n",
        "        Z = np.dot(X_bias, self.theta_)\n",
        "        return self._softmax(Z)\n",
        "\n",
        "    def predict(self, X, y=None):\n",
        "        try:\n",
        "            getattr(self, \"theta_\")\n",
        "        except AttributeError:\n",
        "            raise RuntimeError(\"You must train classifer before predicting data!\")\n",
        "        #Indexes of the maximal elements of a 2-dimensional array\n",
        "        return np.argmax(self.predict_proba(X, y), axis=1)\n",
        "\n",
        "    def fit_predict(self, X, y=None):\n",
        "        self.fit(X, y)\n",
        "        return self.predict(X, y)\n",
        "\n",
        "    def score(self, X, y=None):\n",
        "        probabilities = self.predict_proba(X, y)\n",
        "        return self._cost_function(probabilities, y)\n",
        "        #pass\n",
        "\n",
        "    def _cost_function(self, probabilities, y):\n",
        "        np.clip(probabilities, self.eps, 1 - self.eps)\n",
        "        m = probabilities.shape[0]\n",
        "        one_hot = self._one_hot(y)\n",
        "        return (-1 / m) * np.sum(one_hot * np.log(probabilities))#+(1- one_hot) * np.log(1-probabilities)\n",
        "\n",
        "    def _one_hot(self, y):\n",
        "        yohe = np.zeros((len(y), self.nb_classes))\n",
        "        for i in range(len(y)):\n",
        "            yohe[i][y[i]] = 1\n",
        "        return yohe\n",
        "\n",
        "    def _softmax(self, z):\n",
        "        expZ = np.exp(z)\n",
        "        return np.divide(expZ, np.sum(expZ, axis=1, keepdims=True))\n",
        "    \n",
        "\n",
        "    def _get_gradient(self, X_bias, y, probas):\n",
        "        return (1/X_bias.shape[0]) * np.dot(X_bias.T, (probas-self._one_hot(y)))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Yv83c1sWXGzq"
      },
      "source": [
        "### 1.1 One-hot-encoding (1 point)\n",
        "\n",
        "En machine learning pour représenter un vecteur de données catégoriques, on utilise le one-hot encoding.\n",
        "\n",
        "Pour un vecteur comportant 5 exemples et 3 catégories différentes, on le représente sous forme d'une matrice de taille 5 par 3. Cette matrice est entièrement remplie de 0 sauf à l'indice correspondant au numéro de la classe pour chaque exemple.\n",
        "\n",
        "\n",
        "Par exemple\n",
        "$ y = \\left(\\begin{array}{cc} \n",
        "0 \\\\\n",
        "0 \\\\\n",
        "1 \\\\\n",
        "2 \\\\\n",
        "1 \\\\\n",
        "\\end{array}\\right) $\n",
        "\n",
        "devient:\n",
        "\n",
        "$ yohe =  \\left(\\begin{array}{cc} \n",
        "1. & 0. & 0.\\\\\n",
        "1. & 0. & 0.\\\\\n",
        "0. & 1. & 0.\\\\\n",
        "0. & 0. & 1.\\\\\n",
        "0. & 1. & 0.\\\\\n",
        "\\end{array}\\right) $\n",
        "\n",
        "\n",
        "#### Implémentation\n",
        "1. Implémentez  la fonction  *`_one_hot()`*  dans SoftmaxClassifier. \n",
        "\n",
        "Utilisez la fonction _testOneHot_ pour vérifier votre implémentation de la fonction. Vous devriez avoir ces résultats:\n",
        "```\n",
        "Premier test\n",
        "[[1. 0. 0. 0. 0. 0.]\n",
        " [0. 1. 0. 0. 0. 0.]\n",
        " [0. 0. 1. 0. 0. 0.]\n",
        " [0. 0. 0. 1. 0. 0.]\n",
        " [0. 0. 0. 0. 1. 0.]\n",
        " [0. 0. 0. 0. 0. 1.]]\n",
        "\n",
        "Deuxième test\n",
        "[[0. 0. 0. 0. 0. 1.]\n",
        " [0. 0. 0. 0. 0. 1.]\n",
        " [0. 0. 0. 0. 0. 1.]\n",
        " [0. 0. 0. 0. 0. 1.]]\n",
        "\n",
        " Troisième test\n",
        "[[1. 0. 0. 0. 0. 0.]\n",
        " [1. 0. 0. 0. 0. 0.]\n",
        " [1. 0. 0. 0. 0. 0.]\n",
        " [1. 0. 0. 0. 0. 0.]]\n",
        " ```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HEwBsNVsjvfs",
        "outputId": "ec0e8d4c-3ab6-497c-c387-2b314c11ea02",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "source": [
        "def testOneHot():\n",
        "    softmax = SoftmaxClassifier()\n",
        "    softmax.nb_classes = 6 # les classes possibles sont donc 0-5\n",
        "\n",
        "    y1= np.array([0,1,2,3,4,5])\n",
        "    y1.shape = (6,1)\n",
        "    print('Premier test')\n",
        "    print(softmax._one_hot(y1))\n",
        "\n",
        "    y2 = np.array([5,5,5,5])\n",
        "    y2.shape = (4,1)\n",
        "    print('\\nDeuxième test')\n",
        "    print(softmax._one_hot(y2))\n",
        "\n",
        "    y3 = np.array([0,0,0,0])\n",
        "    y3.shape = (4,1)\n",
        "    print('\\nTroisième test')\n",
        "    print(softmax._one_hot(y3))\n",
        "\n",
        "\n",
        "testOneHot()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Premier test\n",
            "[[1. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 1.]]\n",
            "\n",
            "Deuxième test\n",
            "[[0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 0. 1.]]\n",
            "\n",
            "Troisième test\n",
            "[[1. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vwxvqYj4XGzu"
      },
      "source": [
        "### 1.2 Matrice de poids (1 point)\n",
        "\n",
        "Soit $ X_{m * n} $ la matrice d'exemple et $ \\Theta _{n*K} $ la matrice de poids avec:\n",
        "\n",
        "* **m** le nombre d'exemples\n",
        "* **n** le nombre de features\n",
        "* **k** le nombre de classes\n",
        "\n",
        "Il est d'usage d'ajouter une colonne supplémentaire à X. Cette colonne est remplie de 1 et se trouve au début de la matrice. Elle représente un biais. Pour prendre en compte ce changement, il faut aussi rajouter une ligne à la matrice $\\Theta$.\n",
        "\n",
        "On obtient X_bias$_{m*(n+1)}$ et $ \\Theta _{(n+1)*K} $\n",
        "\n",
        "\n",
        "Intuitivement, à chaque classe K est associée une colonne de $\\theta$.\n",
        "\n",
        "On note $\\theta_k$ le vecteur de dimension n+1 la colonne de poids associée à la prédiction de la classe k.\n",
        "\n",
        "$\\Theta$ = [$\\theta_0$,$\\theta_1$... $\\theta_k$]\n",
        "\n",
        "Ainsi $ z = x * \\Theta $ donne un vecteur de dimension K qui correspond aux **logits** associés à x pour chacune des classes.\n",
        "\n",
        "#### Implémentation\n",
        "1. Instanciez X_bias dans les fonctions  *`fit()`* et `predict_proba()`.\n",
        "2. initialisez $\\Theta$ aléatoirement dans la fonction  *`fit()`*.\n",
        "3. Instanciez aussi le nombre de features et de classe de SoftmaxClassifier dans la fonction  *`fit()`*.\n",
        "\n",
        "**Notez que votre fonction fit n'est pas encore complète.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6UTIxaGIXGzz"
      },
      "source": [
        "### 1.3 Softmax (1 point)\n",
        "\n",
        "On veut convertir le vecteur de logits **z** obtenu dans la partie précédente, en un **vecteur de probabilité**.\n",
        "\n",
        "Pour cela on définit la **fonction softmax**:\n",
        "\n",
        "$$ \\hat{p_x}^k = softmax(z)_k = \\frac{exp(z_k)}{\\sum_{\\substack{1<j<K}} exp(z_j)} $$\n",
        "\n",
        "Intuitivement, pour un logit de z, $z_k$, on prend l'exponentielle de cette valeur et on la divise par la somme des exponentielles de chaque logit du vecteur **z**. On obtient  $\\hat{p_x}^k$ la probabilité que l'exemple **x** appartienne à la classe **k**.\n",
        "\n",
        "On réitère l'opération pour chaque logit du vecteur **z**. \n",
        "\n",
        "On obtient ainsi un vecteur de probabilités $\\hat{p_x}$ pour un exemple **x**. \n",
        "\n",
        "La division permet de rendre la somme des termes du vecteur $\\hat{p_x}$ égale à 1 ce qui est indispensable dans le cadre des probabilités.\n",
        "\n",
        "#### Implémentation\n",
        "1. Implémentez  la fonction  *`_softmax()`*  dans SoftmaxClassifier. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "g5_N31uPXGz2"
      },
      "source": [
        "### 1.4 Prédictions (1 point)\n",
        "\n",
        "Maintenant que vous avez implémentée la fonction **_softmax**, vous pouvez implémenter  les fonctions  **predict_proba** et **predict**  dans SoftmaxClassifier. \n",
        "\n",
        "#### Implémentation\n",
        "1. Implémenter la fonction *`predict_proba()`* de la classe SoftmaxClassifier. Cette fonction retourne les probabilités associées à chaque classe pour chacune des instances à prédire.\n",
        "\n",
        "2. Implémenter la fonction *`predict()`* de la classe SoftmaxClassifier. Cette fonction retourne la classe avec la plus grande probabilité pour chacunes des intances à prédire."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cj5Ghd5jXGz6"
      },
      "source": [
        "### 1.5 Fonction de coût Log loss (2 points)\n",
        "\n",
        "Soit la fonction de coût log loss (ou cross entropy):\n",
        "\n",
        "$$ J( \\Theta) = \\frac{-1}{m}\\sum_{\\substack{1<i<m}} \\sum_{\\substack{1<k<K}} y_k^i log( \\hat{p_k}^i ) $$\n",
        "\n",
        "avec:\n",
        "* **K** le nombre de classes\n",
        "* **m** le nombre d'exemples dans les données\n",
        "* $ \\hat{p_k}^i  $  la probabilité que l'exemple i soit de la classe k\n",
        "* $y_k^i$ vaut 1 si la classe cible de l'exemple i est k, 0 sinon\n",
        "\n",
        "Le coût correspond donc à la moyenne multiplié par -1 des sommes des multiplications des probabilités pour chaque classe pour chaque élement. Autrement dit, plus le coût est petit, plus les prédictions correspondent bien aux étiquettes.\n",
        "\n",
        "\n",
        "**Détail d'implémentation:** La fonction n'est pas définie pour des valeurs de probabilité de 0 ou 1. Il faut donc s'assurer que étant donné $\\epsilon$, les probabilités sont comprises dans [$\\epsilon$, 1. - $\\epsilon$].\n",
        "#### Implémentation\n",
        "1. Implémentez  la fonction  *`_cost_function()`*  dans SoftmaxClassifier en prenant en utilisant la variable self.eps. Utilisez le format one-hot pour représenter $y_k^i$.\n",
        "2.  Utilisez *`_cost_function()`* pour calculer la variable *`loss`* dans la fonction `fit()`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vODCJbRaXGz-"
      },
      "source": [
        "### 1.6 Gradient de la fonction de coût (1 point)\n",
        "\n",
        "Le **gradient de J** par rapport à la classe k (par rapport à $\\theta_k$) est :\n",
        "\n",
        "\n",
        "$$ \\Delta_{\\theta_k}J( \\Theta) = \\frac{1}{m} \\sum_{\\substack{1<i<m}}( \\hat{p_k}^i - y_k^i)x^i  $$\n",
        "\n",
        "avec:\n",
        "* **K** le nombre de classes\n",
        "* **m** le nombre d'exemples dans les données\n",
        "* $ \\hat{p_k}^i  $  la probabilité que l'exemple i soit de la classe k\n",
        "* $y_k^i$ vaut 1 si la classe cible de l'exemple i est k, 0 sinon\n",
        "\n",
        "Sous **forme matricielle**, on peut écrire le **gradient de J par rapport à $\\Theta$**:\n",
        "$$ \\Delta_J( \\Theta) = \\frac{1}{m} X_{bias}^T *( \\hat{p} - y_{ohe}) $$\n",
        "\n",
        "avec:\n",
        "* $\\hat{p}$ la matrice de probabilité prédite pour chaque example et pour chaque classe\n",
        "* $y_{ohe}$ la version one-hot de y\n",
        "* $X_{bias}^T$  la matrice transposée de $X_{bias}$\n",
        "* **\\*** le produit matriciel\n",
        "\n",
        "#### Implémentation\n",
        "1. Implémentez  la fonction  *`_get_gradient()`*  dans SoftmaxClassifier en vous basant sur la forme matricielle du gradient."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Y5EOpj9TXG0B"
      },
      "source": [
        "### 1.7 Mise à jour des poids (1 point)\n",
        "\n",
        "Quand le gradient a été calculé, il faut mettre à jour les poids avec ces gradients.\n",
        "\n",
        "$$ \\Theta  = \\Theta - \\gamma \\Delta J( \\Theta) $$\n",
        "\n",
        "\n",
        "avec:\n",
        "* $\\Theta$ la matrice de poids\n",
        "* $\\gamma$  le taux d'apprentissage\n",
        "* $\\Delta J( \\Theta)$ le gradient de $J( \\Theta)$ selon $\\Theta$\n",
        "\n",
        "#### Implémentation\n",
        "1. Mettez à jour la variable **self.theta_** dans la fonction `fit()`  dans SoftmaxClassifier."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9QrP9uYPXG0I"
      },
      "source": [
        "### 1.8 Évaluation (1 point)\n",
        "\n",
        "Quand on veut évaluer la performance du modèle **après entrainement**, on utilise la fonction de coût. Autrement dit, on compare les prédictions avec les vrais valeurs et calcule un score.\n",
        "\n",
        "#### Implémentation\n",
        "1. Implémentez la fonction `score()` qui permet d'évaluer la qualité de la prédiction **après entrainement** dans SoftmaxClassifier. Elle utilise le log loss comme métrique."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Ewh3I_jEXG0N"
      },
      "source": [
        "### 1.9 Early stopping (1 point)\n",
        "\n",
        "Un trop grand nombre d'**epoch** peut résulter en **overfitting**.\n",
        "Pour pallier à ce problème, on peut utiliser le mécanisme d'**early stopping**.\n",
        "Il s'agit d'arrêter l'entraînement si la différence de la fonction de coût entre deux **epochs consécutives** est inférieure à un **seuil**.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#### Implémentation\n",
        "\n",
        "1. Ajouter le mécanisme d'**early stopping**  quand le booléen **self.early_stopping** est vrai dans la fonction `fit()`. Le seuil est donné par la variable **self.threshold**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JdNEVRveXG0Q"
      },
      "source": [
        "### 1.10 Test de la solution (2 points)\n",
        "\n",
        "Le code ci-dessous importe le dataset de classification multiclasse **iris** disponible sur sklearn. Les données sont divisées en deux parties, l'ensemble d'entraînement et l'ensemble de test, puis elles sont normalisées.\n",
        "\n",
        "Le classifier est entrainé sur l'ensemble d'entrainement et testé sur l'ensemble de test.\n",
        "\n",
        "Le but de cette partie est juste de vérifier votre implémentation **quand vous êtes sûrs que votre code fonctionne**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1-2UipMRXG0R",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# load dataset\n",
        "data,target =load_iris().data,load_iris().target\n",
        "\n",
        "# split data in train/test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split( data, target, test_size=0.33, random_state=42)\n",
        "\n",
        "# standardize columns using normal distribution\n",
        "# fit on X_train and not on X_test to avoid Data Leakage\n",
        "s = StandardScaler()\n",
        "X_train = s.fit_transform(X_train)\n",
        "X_test = s.transform(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8qnflIdzXG0T",
        "colab": {}
      },
      "source": [
        "cl = SoftmaxClassifier()\n",
        "\n",
        "# train on X_train and not on X_test to avoid overfitting\n",
        "train_p = cl.fit_predict(X_train,y_train)\n",
        "test_p = cl.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hFt-r3nQXG0V"
      },
      "source": [
        "Si vous obtenez des valeurs relativement proches pour l'ensemble de test et d'entrainement, et qu'elles sont au moins supérieures à 0.8, votre modèle devrait être correct"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GmqcqadpXG0W",
        "outputId": "73814994-26ea-4a81-c122-35544ea98e01",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "\n",
        "# display precision, recall and f1-score on train/test set\n",
        "print(\"train : \"+ str(precision_recall_fscore_support(y_train, train_p,average = \"macro\")))\n",
        "print(\"test : \"+ str(precision_recall_fscore_support(y_test, test_p,average = \"macro\")))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train : (0.9729729729729729, 0.9714285714285714, 0.9709901198234182, None)\n",
            "test : (0.9791666666666666, 0.9791666666666666, 0.978494623655914, None)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "r3FmllqgXG0d"
      },
      "source": [
        "# 2. Data preprocessing (6 points)\n",
        "\n",
        "##  Kaggle \n",
        "Kaggle est un site dédié au machine learning. On y retrouve un grand nombre de dataset.\n",
        "Des compétitions sont organisées par des organisations. Ces dernières fournissent un dataset et un objectif. Les \"kagglers\" qui participent à ces compétitions soumettent leurs résultats en ligne. Il y a souvent des prix ou des emplois pour ceux qui obtiennent les meilleurs résultats.\n",
        "\n",
        "Il s'agit d'un bon moyen pour développer ses compétences en machine learning sur des vrais datasets.\n",
        "\n",
        "Vous devez créer un compte et formez une équipe sur Kaggle: https://www.kaggle.com/. La compétition pour le TP se trouve à l'adresse suivante: https://www.kaggle.com/c/tp3-inf8215-a19.\n",
        "\n",
        "\n",
        "## Income dataset\n",
        "Le dataset que nous utiliserons est le \"Income dataset\" disponible sur le site de la compétition. C'est une version modifiée du dataset \"Adult\" de UCI qui se trouve à cette adresse: https://archive.ics.uci.edu/ml/datasets/Adult.\n",
        "\n",
        "Il s'agit d'un problème de **classification binaire** du salaire de la personne. Le but est de prédire dans quel catégorie ils appartiennent:\n",
        "* $\\lt=50K$\n",
        "* $\\gt50K$\n",
        "\n",
        "Pour plus d'informations sur les données, lisez la description sur kaggle.\n",
        "\n",
        "## Déroulement d'un projet de machine learning\n",
        "\n",
        "Le but de la suite de ce TP est de vous faire étudier une version simplifiée d'un projet complet de machine learning:\n",
        "\n",
        "1. Nettoyage des données, traitement des valeurs manquantes\n",
        "2. Mise en forme des données pour pouvoir les utiliser dans les algorithmes de machine learning\n",
        "3. Feature engineering: transformation ou combinaison de features entre elles\n",
        "4. Comparaison des performances des différents choix effectués lors du traîtement des données\n",
        "5. Comparaison des performances de différents modèles (dont celui implémenté en première partie)\n",
        "6. Optimisation des hyper-paramètres\n",
        "\n",
        "## Scikit-learn\n",
        "http://scikit-learn.org/stable/\n",
        "\n",
        "Il s'agit d'une bibliothèque de machine learning et data mining, elle propose des outils pour l'analyse et le traîtement des données,  des algorithmes classiques de machine learning comme les réseaux de neuronnes, la régression logistique, les SVM ou autre, enfin des outils permettant de comparer les modèles entre eux comme la cross validation.\n",
        "\n",
        "## Pandas\n",
        "\n",
        "Une bibliothèque permettant de stocker des données et de les manipuler facilement\n",
        "\n",
        "Les deux éléments de base de pandas sont le dataframe et la serie.\n",
        "\n",
        "https://pandas.pydata.org/pandas-docs/version/0.23/generated/pandas.DataFrame.html\n",
        "\n",
        "## Data processing tutorial\n",
        "\n",
        "**Avant de continuer le TP**, familiarisez-vous avec le **pré-traitement des données**, **pandas** et **scikit-learn**, un tutoriel est disponible dans le fichier: **data_processing_tutorial.ipynb**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uJxSGCnOXG0e"
      },
      "source": [
        "## Dataset\n",
        "\n",
        "#### Chargement de l'ensemble d'entraînement et de l'ensemble de test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KRK8EeSZNKLk",
        "colab_type": "code",
        "outputId": "d3de4931-f7c5-4185-e79b-3052c545275e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "#in case we are running the notebook on the colab uncomment this\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KrmSiTseXG0f",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "PATH = \"drive/My Drive/Colab Notebooks/TP3_data/\" # changer le path avec votre path\n",
        "X_train = pd.read_csv(PATH + \"train.csv\")\n",
        "X_test = pd.read_csv(PATH + \"test.csv\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "996_M0-sXG0w"
      },
      "source": [
        "#### 5 premiers exemples de l'ensemble d'entraînement"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "s2U3Xs73-rDr",
        "outputId": "c5c5abc7-41d3-4668-cab3-3d142839abc8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        }
      },
      "source": [
        "X_train.head()"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>Age</th>\n",
              "      <th>Workclass</th>\n",
              "      <th>Final weight</th>\n",
              "      <th>Education</th>\n",
              "      <th>Marital-status</th>\n",
              "      <th>Occupation</th>\n",
              "      <th>Relationship</th>\n",
              "      <th>Sex</th>\n",
              "      <th>Capital-gain</th>\n",
              "      <th>Capital-loss</th>\n",
              "      <th>Hours per week</th>\n",
              "      <th>Native country</th>\n",
              "      <th>Income</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>36721</td>\n",
              "      <td>36</td>\n",
              "      <td>Private</td>\n",
              "      <td>31023</td>\n",
              "      <td>Masters</td>\n",
              "      <td>Married-civ-spouse</td>\n",
              "      <td>Exec-managerial</td>\n",
              "      <td>Husband</td>\n",
              "      <td>Male</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>50</td>\n",
              "      <td>United-States</td>\n",
              "      <td>&gt;50K.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2638</td>\n",
              "      <td>52</td>\n",
              "      <td>?</td>\n",
              "      <td>159755</td>\n",
              "      <td>Assoc-voc</td>\n",
              "      <td>Married-civ-spouse</td>\n",
              "      <td>?</td>\n",
              "      <td>Husband</td>\n",
              "      <td>Male</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>50</td>\n",
              "      <td>United-States</td>\n",
              "      <td>&gt;50K</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>36214</td>\n",
              "      <td>36</td>\n",
              "      <td>Private</td>\n",
              "      <td>359001</td>\n",
              "      <td>Assoc-voc</td>\n",
              "      <td>Divorced</td>\n",
              "      <td>Craft-repair</td>\n",
              "      <td>Not-in-family</td>\n",
              "      <td>Male</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>United-States</td>\n",
              "      <td>&lt;=50K.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>27010</td>\n",
              "      <td>42</td>\n",
              "      <td>Self-emp-inc</td>\n",
              "      <td>188615</td>\n",
              "      <td>Bachelors</td>\n",
              "      <td>Married-civ-spouse</td>\n",
              "      <td>Sales</td>\n",
              "      <td>Husband</td>\n",
              "      <td>Male</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>60</td>\n",
              "      <td>United-States</td>\n",
              "      <td>&gt;50K</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>27506</td>\n",
              "      <td>44</td>\n",
              "      <td>Private</td>\n",
              "      <td>43711</td>\n",
              "      <td>Bachelors</td>\n",
              "      <td>Never-married</td>\n",
              "      <td>Other-service</td>\n",
              "      <td>Not-in-family</td>\n",
              "      <td>Male</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>48</td>\n",
              "      <td>United-States</td>\n",
              "      <td>&lt;=50K</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   index  Age      Workclass  ...  Hours per week  Native country   Income\n",
              "0  36721   36        Private  ...              50   United-States    >50K.\n",
              "1   2638   52              ?  ...              50   United-States     >50K\n",
              "2  36214   36        Private  ...              40   United-States   <=50K.\n",
              "3  27010   42   Self-emp-inc  ...              60   United-States     >50K\n",
              "4  27506   44        Private  ...              48   United-States    <=50K\n",
              "\n",
              "[5 rows x 14 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kSM01gVZXG0z"
      },
      "source": [
        "#### 5 premiers exemples de l'ensemble de test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UMVNvQ9yXG00",
        "outputId": "9d31874c-7d4e-4bb2-a0a3-01be55b6e32e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        }
      },
      "source": [
        "X_test.head()"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>Age</th>\n",
              "      <th>Workclass</th>\n",
              "      <th>Final weight</th>\n",
              "      <th>Education</th>\n",
              "      <th>Marital-status</th>\n",
              "      <th>Occupation</th>\n",
              "      <th>Relationship</th>\n",
              "      <th>Sex</th>\n",
              "      <th>Capital-gain</th>\n",
              "      <th>Capital-loss</th>\n",
              "      <th>Hours per week</th>\n",
              "      <th>Native country</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>7762</td>\n",
              "      <td>18</td>\n",
              "      <td>Private</td>\n",
              "      <td>423024.0</td>\n",
              "      <td>HS-grad</td>\n",
              "      <td>Never-married</td>\n",
              "      <td>Other-service</td>\n",
              "      <td>Not-in-family</td>\n",
              "      <td>Male</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>United-States</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>23881</td>\n",
              "      <td>17</td>\n",
              "      <td>Private</td>\n",
              "      <td>178953.0</td>\n",
              "      <td>12th</td>\n",
              "      <td>Never-married</td>\n",
              "      <td>Sales</td>\n",
              "      <td>Own-child</td>\n",
              "      <td>Female</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>United-States</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>30507</td>\n",
              "      <td>25</td>\n",
              "      <td>Local-gov</td>\n",
              "      <td>348986.0</td>\n",
              "      <td>HS-grad</td>\n",
              "      <td>Never-married</td>\n",
              "      <td>Handlers-cleaners</td>\n",
              "      <td>Other-relative</td>\n",
              "      <td>Male</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>United-States</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>28911</td>\n",
              "      <td>20</td>\n",
              "      <td>Private</td>\n",
              "      <td>218215.0</td>\n",
              "      <td>Some-college</td>\n",
              "      <td>Never-married</td>\n",
              "      <td>Sales</td>\n",
              "      <td>Own-child</td>\n",
              "      <td>Female</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>United-States</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>19484</td>\n",
              "      <td>47</td>\n",
              "      <td>Private</td>\n",
              "      <td>244025.0</td>\n",
              "      <td>HS-grad</td>\n",
              "      <td>Never-married</td>\n",
              "      <td>Machine-op-inspct</td>\n",
              "      <td>Unmarried</td>\n",
              "      <td>Male</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>56.0</td>\n",
              "      <td>Puerto-Rico</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   index  Age   Workclass  ...  Capital-loss Hours per week  Native country\n",
              "0   7762   18     Private  ...           0.0           20.0   United-States\n",
              "1  23881   17     Private  ...           0.0           20.0   United-States\n",
              "2  30507   25   Local-gov  ...           0.0           40.0   United-States\n",
              "3  28911   20     Private  ...           0.0           30.0   United-States\n",
              "4  19484   47     Private  ...           0.0           56.0     Puerto-Rico\n",
              "\n",
              "[5 rows x 13 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vli9bS98XG04"
      },
      "source": [
        "#### 5 premiers exemples de l'attribut à prédire"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HlgNDI3BXG05",
        "outputId": "30226162-e727-43cc-cc71-59e7c7d1c02a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "y_train = X_train['Income']\n",
        "y_train.head()"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0      >50K.\n",
              "1       >50K\n",
              "2     <=50K.\n",
              "3       >50K\n",
              "4      <=50K\n",
              "Name: Income, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qkm_PxPOXG08"
      },
      "source": [
        "## Travail demandé\n",
        "\n",
        "En vous appuyant sur le tutoriel fourni, vous devez écrire un pipeline complet de transformation pour chacune des colonnes du dataset.\n",
        "\n",
        "Vous êtes **libres** de vos choix, mais vous devez les **justifer** colonne par colonne.\n",
        "Par exemple, vous pouvez choisir de combiner des colonnes entre elles, de séparer une colonne en plusieurs ou encore d'éliminer complètement une colonne si vous le justifiez correctement.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HOX3V-pbBNYm"
      },
      "source": [
        "####  2.1 Preprocessing (3 points)\n",
        "\n",
        "Vous devez justifier toutes les transformations que vous faites sur les données pour faciliter l'utilisation des données lors de l'entraînement du modèle. Vous pouvez justifier avec de la logique ou des preuves numériques. **Attention de ne pas tomber dans le piège du overfitting**.\n",
        "\n",
        "**ATTENTION** Vous devez avoir un meilleur score que le baseline pour le classement privé pour avoir votre point au complet pour la partie 2.1. Ce score est une bonne mesure pour savoir si votre traitement est correct.\n",
        "\n",
        "1. Justifier tous les choix de traitements que vous faites ici."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bjKBBvUMLpFD"
      },
      "source": [
        "\n",
        "Colonne(s) | Traitement | Justification |\n",
        " :---: | :---: | :---: |\n",
        " Index | Nous l'enlevons complètement. | Aucun effet sur la valeur 'income' |\n",
        " Age | Nous avons normalisé les valeurs | Nous voulions mettre toutes les valeurs numériques à la même échelle |\n",
        " Workclass | Nous avons converti les catégories en 4 valeurs binaires | Les valeurs sont catégorielles et textuelles. Nous avons regroupés les travailleurs du secteur public et ceux autonomes en 2 groupes, car ces secteurs sont similaires.  |\n",
        " Final weight | Nous l'enlevons complètement. | Aucun effet sur la valeur 'income' |\n",
        " Education | Nous avons converti les catégories en 8 valeurs binaires | Les valeurs sont catégorielles et textuelles. Aussi, nous avons regroupé tous ceux avec une éduction High School ou plus bas, car nous avons jugé que c'était similaire |\n",
        " Marital-Status | Nous avons converti les catégories en 5 valeurs binaires | Les valeurs sont catégorielles et textuelles. Aussi, nous avons regroupé tous ceux étant mariés, car similaires |\n",
        " Occupation | Nous avons converti les catégories en 15 valeurs binaires | Les valeurs sont catégorielles et textuelles |\n",
        " Relationship | Nous avons converti les catégories en 7 valeurs binaires | Les valeurs sont catégorielles et textuelles |\n",
        " Sex | Nous avons converti les catégories en 2 valeurs binaires | Les valeurs sont binaires (homme ou femme) et textuelles |\n",
        " Capital Gain | Nous avons normalisé les valeurs | Nous voulions mettre toutes les valeurs numériques à la même échelle |\n",
        " Capital Loss | Nous avons normalisé les valeurs| Nous voulions mettre toutes les valeurs numériques à la même échelle |\n",
        " Hours per week | Nous avons normalisé les valeurs| Nous voulions mettre toutes les valeurs numériques à la même échelle |\n",
        " Native Country | Nous avons converti les catégories en 2 valeurs binaires | Les valeurs sont catégorielles et textuelles. Nous avons choisi de conserver seulement les États-Unies et Autres |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ssOZnxpIXG1K"
      },
      "source": [
        "### 2.2 Pipeline (3 points)\n",
        "Pour faciliter le processus du traitement de données, une pipeline est utilisée. Cela permet de rapidement modifier les transformations appliquées sur l'ensemble de données.\n",
        "\n",
        "#### Implémentation\n",
        "1. Implémenter et associer vos méthodes de traitements a des pipelines.\n",
        "2. Mettez tous vos pipelines dans le ColumnTransformer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7jD82yEyW8PQ",
        "colab": {}
      },
      "source": [
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "## Wrapper pour vous aider pour les pipelines\n",
        "class TransformationWrapper(BaseEstimator,TransformerMixin):\n",
        "    \n",
        "    def __init__(self,fitation= None, transformation = None): \n",
        "        \n",
        "        self.transformation = transformation\n",
        "        self.fitation = fitation\n",
        "        \n",
        "    \n",
        "        \n",
        "    def fit(self, X, y=None):\n",
        "        X = pd.DataFrame(X)\n",
        "        self.data_ = None\n",
        "        self.column_name_ = X.columns[0]\n",
        "        if self.fitation == None:\n",
        "            return self\n",
        "        \n",
        "        self.data_ = [self.fitation(X[self.column_name_])]\n",
        "        return self  \n",
        "    \n",
        "    def transform(self, X, y=None): \n",
        "        X = pd.DataFrame(X)\n",
        "        \n",
        "        if self.data_ != None:\n",
        "            return pd.DataFrame(X.apply(\n",
        "                lambda row: pd.Series( self.transformation(row[self.column_name_], self.data_)),\n",
        "                axis = 1\n",
        "            ))\n",
        "        else:\n",
        "            return pd.DataFrame(X.apply(\n",
        "                lambda row: pd.Series( self.transformation(row[self.column_name_])),\n",
        "                axis = 1\n",
        "            ))\n",
        "        \n",
        "        \n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "class LabelEncoderP(LabelEncoder):\n",
        "    def fit(self, X, y=None):\n",
        "        super(LabelEncoderP, self).fit(X)\n",
        "    def transform(self, X, y=None):\n",
        "        return pd.DataFrame(super(LabelEncoderP, self).transform(X))\n",
        "    def fit_transform(self, X, y=None):\n",
        "        return super(LabelEncoderP, self).fit(X).transform(X)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QgJNaKhbE1RB",
        "outputId": "83c39383-4dee-40fe-c1c9-3ed58b2a11db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# Implémenter ici les différentes transformations customs ici pour que cela soit plus claires (Si vous en avez)\n",
        "\n",
        "def parse_native_country(text):\n",
        "    if \"United-States\" not in text:\n",
        "        return \"Other\"\n",
        "    return text\n",
        "\n",
        "\n",
        "def parse_education(text):\n",
        "    if \"1st\" in text \\\n",
        "            or \"5th\" in text \\\n",
        "            or \"7th\" in text \\\n",
        "            or \"8th\" in text \\\n",
        "            or \"9th\" in text \\\n",
        "            or \"10th\" in text \\\n",
        "            or \"11th\" in text \\\n",
        "            or \"12th\" in text \\\n",
        "            or \"HS\" in text \\\n",
        "            or \"Preschool\" in text:\n",
        "        return \"HS or lower\"\n",
        "    return text\n",
        "\n",
        "\n",
        "def parse_unknown(text):\n",
        "    if \"?\" in text:\n",
        "        return \"Unknown\"\n",
        "    return text\n",
        "\n",
        "\n",
        "\n",
        "def parse_married(text):\n",
        "    if \"Married\" in text:\n",
        "        return \"Married\"\n",
        "    return text\n",
        "\n",
        "\n",
        "def parse_income(text):\n",
        "    if \".\" in text:\n",
        "        return text[:-1]\n",
        "    return text\n",
        "\n",
        "\n",
        "def parse_workclass(text_label):\n",
        "  if \" Self-emp-not-inc\" == text_label:\n",
        "    return \"Self-employed-not-inc\"\n",
        "  elif \" Self-emp-inc\" == text_label:\n",
        "    return \"Self-employed-inc\"\n",
        "  elif \"Private\" in text_label:\n",
        "    return \"Private\"\n",
        "  elif \"gov\" in text_label:\n",
        "    return \"Government\"\n",
        "  else:\n",
        "    return \"Other\"\n",
        "\n",
        "def parse_hours_per_week(hpw):\n",
        "  if hpw < 40:\n",
        "    return 1.0\n",
        "  elif hpw >= 40 and hpw < 45 :\n",
        "    return 2.0\n",
        "  elif hpw >= 45 and hpw < 60 :\n",
        "    return 3.0\n",
        "  elif hpw >= 60 and hpw < 80 :\n",
        "    return 4.0\n",
        "  else: #if hpw >= 80:\n",
        "    return 5.0\n",
        "\n",
        "\"\"\"\n",
        "def parse_workclass(text):\n",
        "    words = text.split(\"-\")\n",
        "    if words[0] == \" Self\":\n",
        "        return \"Self-employed\"\n",
        "    elif words[0] == \" Private\":\n",
        "        return \"Private\"\n",
        "    elif len(words) > 1:\n",
        "        if words[1] == \"gov\":\n",
        "            return \"Public\"\n",
        "    return \"Unknown\"\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"\n",
        "def parse_marital_status(text_label):\n",
        "  if \" Married-civ-spouse\" == text_label:\n",
        "    return \"Married\"\n",
        "  elif \" Married-AF-spouse\" == text_label:\n",
        "    return \"Married\"\n",
        "  else:\n",
        "    return \"Unmarried\"\n",
        "\n",
        "def parse_relationship(text_label):\n",
        "  if \"Husband\" in text_label:\n",
        "    return \"couple\"\n",
        "  elif \"Wife\" in text_label:\n",
        "    return \"couple\"\n",
        "  else:\n",
        "    return \"single\"\n",
        "\"\"\""
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ndef parse_marital_status(text_label):\\n  if \" Married-civ-spouse\" == text_label:\\n    return \"Married\"\\n  elif \" Married-AF-spouse\" == text_label:\\n    return \"Married\"\\n  else:\\n    return \"Unmarried\"\\n\\ndef parse_relationship(text_label):\\n  if \"Husband\" in text_label:\\n    return \"couple\"\\n  elif \"Wife\" in text_label:\\n    return \"couple\"\\n  else:\\n    return \"single\"\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "N97zWoWQXG1K",
        "colab": {}
      },
      "source": [
        "from sklearn.pipeline import Pipeline, FeatureUnion\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "\n",
        "pipeline_workclass = Pipeline([\n",
        "    (\"fillna\", SimpleImputer(missing_values = ' ?', strategy = 'constant', fill_value='Unknown')),\n",
        "    ('workclass', TransformationWrapper(transformation=parse_workclass)),\n",
        "    (\"encode\", LabelEncoderP()),\n",
        "    (\"oneHotEncode\", OneHotEncoder(categories='auto', sparse=False))\n",
        "])\n",
        "\n",
        "pipeline_income = Pipeline([\n",
        "    ('income', TransformationWrapper(transformation=parse_income))\n",
        "])\n",
        "\n",
        "# pour normaliser\n",
        "pipeline_numerical = Pipeline([\n",
        "    (\"fillna\", SimpleImputer(strategy='mean')),\n",
        "    (\"scaler\", StandardScaler())\n",
        "])\n",
        "#hours_per_week\n",
        "pipeline_hours_per_week = Pipeline([\n",
        "    ('Hours_per_week', TransformationWrapper(transformation = parse_hours_per_week)),\n",
        "    (\"encode\",OneHotEncoder(categories = 'auto', sparse = False))\n",
        "])\n",
        "\n",
        "# pour binariser\n",
        "pipeline_hot_encode = Pipeline([\n",
        "    (\"encode\", OneHotEncoder(categories='auto', sparse=False))\n",
        "])\n",
        "\n",
        "pipeline_categorical = Pipeline([\n",
        "    (\"encode\", LabelEncoderP())\n",
        "])\n",
        "\n",
        "pipeline_education = Pipeline([\n",
        "    ('education', TransformationWrapper(transformation=parse_education)),\n",
        "    (\"encode\", LabelEncoderP()),\n",
        "    (\"oneHotEncode\", OneHotEncoder(categories='auto', sparse=False))\n",
        "])\n",
        "\n",
        "pipeline_country = Pipeline([\n",
        "    ('native_country', TransformationWrapper(transformation=parse_native_country)),\n",
        "    (\"encode\", LabelEncoderP()),\n",
        "    (\"oneHotEncode\", OneHotEncoder(categories='auto', sparse=False))\n",
        "])\n",
        "\n",
        "pipeline_married = Pipeline([\n",
        "    ('marital_status', TransformationWrapper(transformation=parse_married)),\n",
        "    (\"encode\", LabelEncoderP()),\n",
        "    (\"oneHotEncode\", OneHotEncoder(categories='auto', sparse=False))\n",
        "])\n",
        "\n",
        "pipeline_occupation = Pipeline([\n",
        "    (\"fillna\", SimpleImputer(missing_values = ' ?', strategy = 'constant', fill_value='Unknown')),\n",
        "    (\"encode\", LabelEncoderP()),\n",
        "    (\"oneHotEncode\", OneHotEncoder(categories='auto', sparse=False))\n",
        "])\n",
        "\n",
        "#Relationship\n",
        "\n",
        "\n",
        "full_pipeline = ColumnTransformer([\n",
        "    (\"Age\", pipeline_numerical, [\"Age\"]),\n",
        "    (\"Final weight\", pipeline_numerical, [\"Final weight\"]),\n",
        "    (\"Workclass\", pipeline_workclass, [\"Workclass\"]),\n",
        "    (\"Education\", pipeline_education, [\"Education\"]),\n",
        "    (\"Marital-status\", pipeline_married, [\"Marital-status\"]),\n",
        "    (\"Occupation\", pipeline_hot_encode, [\"Occupation\"]),\n",
        "    (\"Relationship\", pipeline_hot_encode, [\"Relationship\"]),\n",
        "    (\"Sex\", pipeline_hot_encode, [\"Sex\"]),\n",
        "    (\"Capital-gain\", pipeline_numerical, [\"Capital-gain\"]),\n",
        "    (\"Capital-loss\", pipeline_numerical, [\"Capital-loss\"]),\n",
        "    (\"Hours per week\", pipeline_hours_per_week, [\"Hours per week\"]),\n",
        "    (\"Native country\", pipeline_country, [\"Native country\"]),\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TcprzRziXG1Q"
      },
      "source": [
        "Lancez le pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1YAV0fTfXG1R",
        "outputId": "49ebe72e-fc21-480c-96ab-9cac33c93a8b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 717
        }
      },
      "source": [
        "column_names = [\"Age\",\n",
        "                \"Final-weight\",\n",
        "                \"Workclass1\", \"Workclass2\", \"Workclass3\", \"Workclass4\",\"Workclass5\",#\"Workclass6\",\n",
        "                \"Education1\", \"Education2\", \"Education3\", \"Education4\",\n",
        "                \"Education5\", \"Education6\", \"Education7\", \"Education8\",\n",
        "                \"MaritalStatus1\", \"MaritalStatus2\", \"MaritalStatus3\", \"MaritalStatus4\", \"MaritalStatus5\",\n",
        "                #\"MaritalStatus6\", \"MaritalStatus7\",\n",
        "                \"Occupation1\", \"Occupation2\",\n",
        "                \"Occupation3\", \"Occupation4\", \"Occupation5\", \"Occupation6\", \"Occupation7\", \"Occupation8\", \"Occupation9\",\n",
        "                \"Occupation10\", \"Occupation11\", \"Occupation12\", \"Occupation13\", \"Occupation14\", \"Occupation15\",\n",
        "                \"Female\", \"Male\",\n",
        "                \"Relationship1\", \"Relationship2\", \"Relationship3\", \"Relationship4\", \"Relationship5\", \"Relationship6\",\n",
        "                \"Capital-gain\", \"Capital-loss\", \n",
        "                #\"Hours per week\",\n",
        "                \"Hours per week 1\",  \"Hours per week 2\", \"Hours per week 3\", \"Hours per week 4\",\"Hours per week 5\",\n",
        "                \"NativeCountry1\", \"NativeCountry2\"\n",
        "                ]\n",
        "X_train_preprocess = pd.DataFrame(full_pipeline.fit_transform(X_train), columns=column_names)\n",
        "X_test_preprocess = pd.DataFrame(full_pipeline.fit_transform(X_test), columns=column_names)\n",
        "print(X_train_preprocess.head())\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/label.py:219: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/label.py:252: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/label.py:219: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/label.py:252: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/label.py:219: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/label.py:252: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/label.py:219: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/label.py:252: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/label.py:219: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/label.py:252: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/label.py:219: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/label.py:252: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/label.py:219: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/label.py:252: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "        Age  Final-weight  ...  NativeCountry1  NativeCountry2\n",
            "0 -0.190154     -1.503257  ...             1.0             0.0\n",
            "1  0.974165     -0.285780  ...             1.0             0.0\n",
            "2 -0.190154      1.598580  ...             1.0             0.0\n",
            "3  0.246465     -0.012838  ...             1.0             0.0\n",
            "4  0.392005     -1.383261  ...             1.0             0.0\n",
            "\n",
            "[5 rows x 52 columns]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/label.py:219: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/label.py:252: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cv-fvZs2GDJD",
        "colab_type": "code",
        "outputId": "7b5f59c7-0686-42c2-9079-800f7ebe0f21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "#new\n",
        "print(X_test[\"Occupation\"].unique())\n",
        "print(len(X_test[\"Occupation\"].unique()))\n",
        "\"\"\"\n",
        "print(np.sort(X_test_preprocess[\"Occupation\"].unique()))\n",
        "print(np.sort(X_train_preprocess[\"Occupation\"].unique()))\n",
        "print(len(X_test_preprocess[\"Occupation\"].unique()))\n",
        "print(len(X_train_preprocess[\"Occupation\"].unique()))\n",
        "\"\"\"\n",
        "len(X_train_preprocess.head())\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[' Other-service' ' Sales' ' Handlers-cleaners' ' Machine-op-inspct'\n",
            " ' Craft-repair' ' Adm-clerical' ' Prof-specialty' ' Exec-managerial' ' ?'\n",
            " ' Transport-moving' ' Farming-fishing' ' Tech-support' ' Protective-serv'\n",
            " ' Priv-house-serv' ' Armed-Forces']\n",
            "15\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jZWQoaa2XG1Z"
      },
      "source": [
        "# 3. Model selection (2 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JxwJyM9gXG1Z"
      },
      "source": [
        "Encodage de la classe cible sous forme d'entiers pour l'utiliser\n",
        "avec les algorithmes de scikit-learn. Vous devez avoir 2 classes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mXfDgss4XG1a",
        "outputId": "045bd805-548d-4508-887d-da5b0546bfc8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "y_train.loc[y_train == ' >50K.' ] = ' >50K'\n",
        "y_train.loc[y_train == ' <=50K.'] = ' <=50K'\n",
        "target_label = LabelEncoder()\n",
        "y_train_label = target_label.fit_transform(y_train)\n",
        "print(y_train_label)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 1 0 ... 0 0 0]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py:205: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  self._setitem_with_indexer(indexer, value)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5ekhKj0cXG1e"
      },
      "source": [
        "## 3.1 Ensemble de validation (0.5 point)\n",
        "Pour comparer différents modèles entre eux, on ne peut pas utiliser\n",
        "l'ensemble de test, sinon on serait tenté de garder le modèle correspondant le mieux à l'ensemble de test ce qui pourrait conduire à l'overfitting.\n",
        "\n",
        "Il est d'usage de créer un nouvel ensemble de la taille de l'ensemble de test, l'ensemble de **validation**.\n",
        "\n",
        "###  Cross-validation\n",
        "\n",
        "La cross-validation est une méthode utile pour comparer la performance de différents modèles de machine learning **sans créer d'ensemble de validation**.\n",
        "\n",
        "Il existe différents types de cross-validation, la procédure la plus classique est la suivante:\n",
        "* Diviser aléatoirement l'ensemble d'entraînement en deux parties (90%/10% par exemple).\n",
        "* Entraîner le modèle sur la plus grande partie, et le tester sur l'autre partie.\n",
        "* Recommencer n fois\n",
        "* Calculer la moyenne et l'écart type des résultats\n",
        "\n",
        "Les avantages sont les suivants:\n",
        "* Considérer la totalité de l'ensemble d'entraînement pour l'évaluation (sans se priver de l'ensemble de validation)\n",
        "* Obtenir l'écart-type des résultats permet une meilleure évaluation de la précision du modèle.\n",
        "\n",
        "L'inconvénient principal est le temps de calcul, étant donné que l'on effectue l'apprentissage du modèle plusieurs fois, cette méthode peut être impossible pour des datasets contenant un grand nombre d'exemple (> 10e5)\n",
        "\n",
        "### Implémentation\n",
        "1. Implémenter la fonction `compare()`. Cette fonction effectue la crossvalidation pour différents modèles et retourne la moyenne et l'écart-type des métriques pour chaque modèle.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WF3S-utAXG1k",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import cross_validate\n",
        "def compare(models, X_train, y_train, nb_runs, scoring):\n",
        "    scores = []\n",
        "    \n",
        "    for model in models:\n",
        "        score = cross_validate(model, X_train, y_train, cv=nb_runs, scoring=scoring)\n",
        "        scores.append(score)\n",
        "    return scores"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GMqcqhGxXG1j"
      },
      "source": [
        "## 3.2 Essai de modèle de sklearn (0.5 point)\n",
        "\n",
        "\n",
        "**Choisir au moins deux modèles permettant la classification binaire sur sklearn en plus du modèle implémenté dans la première partie du TP**.\n",
        "\n",
        "**En vous basant sur les différentes métriques retournées par la fonction `compare()`, concluez quant au modèle le plus performant.**\n",
        "\n",
        "Evaluez les modèles pour les différentes métriques proposées:\n",
        "* **log loss**: c'est la métrique d'évaluation de kaggle\n",
        "* **precision**: correspond à la qualité de la prédiction, le nombre de classes correctement prédites par le nombre de prédiction total\n",
        "* **recall**: le nombre d'éléments appartenant à une classe, identifiés comme tel, divisé par le nombre total des éléments de cette classe.\n",
        "* **f-score**: une moyenne de la precision et du recall\n",
        "\n",
        "**Remarque: precision et recall sont deux mesures complémentaires pour l'évaluation d'un modèle de classification.**\n",
        "\n",
        "Dans le cas d'une classification binaire avec un déséquilibre de la classe cible important, (90%/10%), en évaluant le résultat de la classification avec l'accuracy (nombre de prédictions correctes divisé par le nombre de prédictions total), on peut obtenir un très bon score (90% d'accuracy) en choisissant de prédire systématiquement la classe majoritaire.\n",
        "\n",
        "Dans un tel cas, la precision serait élevée de même, mais le recall serait très bas , nous indiquant la médiocrité de notre modèle.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e78hGscXTHyS",
        "colab_type": "code",
        "outputId": "b0aa35cd-12e9-42ea-c5b6-69fdb9fef805",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360
        }
      },
      "source": [
        "!pip install catboost"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting catboost\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3d/f6/733fe7cca5d0d882e1a708ad59da2510416cc2e4fa54e17c7a5082f67811/catboost-0.20.1-cp36-none-manylinux1_x86_64.whl (63.6MB)\n",
            "\u001b[K     |████████████████████████████████| 63.6MB 37kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from catboost) (1.17.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from catboost) (3.1.2)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.6/dist-packages (from catboost) (4.1.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from catboost) (1.12.0)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.6/dist-packages (from catboost) (0.10.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from catboost) (1.3.3)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.6/dist-packages (from catboost) (0.25.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->catboost) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->catboost) (1.1.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->catboost) (2.4.5)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->catboost) (2.6.1)\n",
            "Requirement already satisfied: retrying>=1.3.3 in /usr/local/lib/python3.6/dist-packages (from plotly->catboost) (1.3.3)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.24.0->catboost) (2018.9)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib->catboost) (42.0.2)\n",
            "Installing collected packages: catboost\n",
            "Successfully installed catboost-0.20.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gSsKSE6cXG1n",
        "colab": {}
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from catboost import CatBoostClassifier\n",
        "\n",
        "\n",
        "\n",
        "nb_run = 3\n",
        "\n",
        "models = [\n",
        "    #SoftmaxClassifier(), # le modele que vous avez implémenté plus haut\n",
        "    #RandomForestClassifier(criterion='entropy', max_depth=7, max_features='auto', n_estimators=500),#10\n",
        "    #GradientBoostingClassifier(n_estimators=250, learning_rate=0.1, max_depth=4, random_state=42),#11\n",
        "    #DecisionTreeClassifier(criterion='entropy', max_depth=7),\n",
        "    #GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=4, random_state=42),#13\n",
        "    #GradientBoostingClassifier(n_estimators=50, learning_rate=0.1, max_depth=4, random_state=42),#0.86279\n",
        "    #GradientBoostingClassifier(n_estimators=56 , learning_rate=0.1,max_depth=8, random_state=42,max_features=8),#0.86450772 parse workclass 2 0.86450772 \n",
        "    #GradientBoostingClassifier(n_estimators=52 , learning_rate=0.1,max_depth=8, random_state=42,max_features=7),#0.8655003 parse workclass 2 + final weight added\n",
        "    #XGBClassifier(colsample_bytree=0.8, learning_rate=0.1, max_depth=3, min_child_weight=1,  n_estimators=1000, random_state=42,subsample=0.8),\n",
        "    #DecisionTreeClassifier(criterion = \"gini\", random_state = 1,max_depth=3, min_samples_leaf=50,min_samples_split=50),\n",
        "    #XGBClassifier(max_depth=3, n_estimators=300, learning_rate=0.01),\n",
        "    #XGBClassifier(max_depth=3, n_estimators=300, learning_rate=0.1),\n",
        "    #GaussianNB(),\n",
        "    #DecisionTreeClassifier(),\n",
        "    #KNeighborsClassifier(),\n",
        "    #MLPClassifier(hidden_layer_sizes=(10,10,10), max_iter=500, solver=\"adam\", learning_rate='adaptive', activation='logistic'),# 0.86090949 \n",
        "    CatBoostClassifier(learning_rate=0.01,depth = 10, l2_leaf_reg=2),\n",
        "    GradientBoostingClassifier(n_estimators=52 , learning_rate=0.1,max_depth=8, random_state=42,max_features=7),\n",
        "    #GradientBoostingClassifier(n_estimators=52 , learning_rate=0.1,max_depth=8, random_state=42,max_features=7),\n",
        "    #CatBoostClassifier(),\n",
        "    #MLPClassifier(hidden_layer_sizes=(10,10,10), max_iter=500, solver=\"adam\", learning_rate='adaptive', activation='logistic'),# 0.86090949 \n",
        "   ]\n",
        "\n",
        "scoring = ['neg_log_loss', 'precision_macro','recall_macro','f1_macro']\n",
        "\n",
        "# parameter_space = {\n",
        "#     'hidden_layer_sizes': [(12,12,12), (13,13,13)],\n",
        "# }\n",
        "# print(\"start\")\n",
        "# clf = GridSearchCV(models[5], parameter_space, n_jobs=-1, cv=3)\n",
        "# clf.fit(X_train_preprocess, y_train_label)\n",
        "\n",
        "# #best results \n",
        "# print('Best parameters found:\\n', clf.best_params_)\n",
        "# # All results\n",
        "# means = clf.cv_results_['mean_test_score']\n",
        "# stds = clf.cv_results_['std_test_score']\n",
        "# for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
        "#     print(\"%0.3f (+/-%0.03f) for %r\" % (mean, std * 2, params))\n",
        "\n",
        "#compare(models, X_train_preprocess, y_train_label, nb_run, scoring)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmWwp-jnqABE",
        "colab_type": "text"
      },
      "source": [
        "#New Section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y-aQAB84p-pb",
        "colab_type": "code",
        "outputId": "1942bc52-2cf6-4018-95e0-aa3d42f6e650",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from sklearn.metrics import f1_score,precision_score,recall_score,confusion_matrix\n",
        "\n",
        "submission_reference = pd.read_csv(PATH + \"submission_reference.csv\")\n",
        "\n",
        "#test_prediction = pd.read_csv(PATH +\"Other/\"+ \"test_prediction_5.csv\")\n",
        "test_prediction = pd.read_csv(PATH + \"test_prediction_1.csv\")\n",
        "\n",
        "#print(test_prediction.head())\n",
        "\n",
        "#pred_test_label = LabelEncoder()\n",
        "#pred_test_label = pred_test_label.fit_transform(pred_test['Income'])  #from locally predicted data\n",
        "#pred_test_label = pred_test_label.fit_transform(test_prediction['Income']) #from external file\n",
        "\n",
        "y_test_label = LabelEncoder()\n",
        "y_test_label = y_test_label.fit_transform(submission_reference[' Income'])\n",
        "\n",
        "# Train selected model\n",
        "i = 0\n",
        "for model in models:\n",
        "  #train\n",
        "  #selected_model = None\n",
        "  #print(model)\n",
        "  selected_model = model\n",
        "  if i == 0 :\n",
        "    selected_model.fit(X_train_preprocess,y_train_label,silent=True)#disable prints for CatBoostClassifier\n",
        "    i = i+1\n",
        "  else:\n",
        "    selected_model.fit(X_train_preprocess,y_train_label)\n",
        "  #selected_model.fit(dummies_train,y_train_label)\n",
        "  \n",
        "  y_pred = selected_model.predict(X_test_preprocess)\n",
        "  #y_pred = selected_model.predict(dummies_test)\n",
        "  #test\n",
        "  #pred_test['Income'] = target_label.inverse_transform(pred_test['Income'])\n",
        "  \n",
        "  print(f\"score = \\t{selected_model.score(X_test_preprocess, y_test_label):.8f} \")\n",
        "  #print('\\nConfusion matrix:\\n',pd.DataFrame(confusion_matrix(y_test_label, y_pred), columns = target_label.classes_, index = target_label.classes_))\n"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "score = \t0.86705131 \n",
            "score = \t0.86401142 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kutzIhLu1frH"
      },
      "source": [
        "Justification pour le meilleur modèle\n",
        "\n",
        "Suite aux résultats obtenus, nous avons choisi le classificateur RandomForestClassifier pour l'apprentissage du dataset. En effet, avec un peu d'ajustement des hyper-paramètres, ce classificateur obtient de bons résultats en un temps très court par rapport aux deux autres. On observe notamment:\n",
        "\n",
        "Le log-loss est le deuxième des trois classificateurs testés.\n",
        "La précision est bien meilleure (~ 0.825) que les deux autres classificateurs.\n",
        "Le recall n'est pas le meilleur, mais se rapproche de celui du MLP tout en étant relativement stable (peu de variance).\n",
        "Le F-score n'est également pas le meilleur ni le plus invariant, mais la rapidité d'entraînement du classificateur est nettement meilleur que celui du MLP.\n",
        "\n",
        "Le classificateur Softmax que nous avons implémenté n'est pas dans la course; le log-loss est très important et la précision n'est pas comparable aux deux autres méthodes utilisées. Pour contrebalancer ce désavantage, il faudrait effectuer de nombreuses epoch (bien plus que 1000), en plus d'ajuster très finement les hyper-paramètres, ce qui demande beaucoup de temps.\n",
        "\n",
        "Sinon, MLP est un classificateur très intéressant, mais qui requiert un temps d'entraînement très long, ce qui rend le testage des hyper-paramètres plus difficile (même en utilisant GridSearchCV pour tester les différents hyper-paramètres)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_th8iyQ2XG1u"
      },
      "source": [
        "## 3.3 Matrice de confusion (0.5 point)\n",
        "\n",
        "La matrice de confusion A est telle que $A_{i,j}$ correspond au nombre d'exemples de la classe i classifié comme appartenant à la classe j.\n",
        "\n",
        "Entrainez le modèle sélectionné sur la totalité de l'ensemble d'entraînement.\n",
        "A l'aide de la matrice de confusion et de la distribution des classes, analysez plus en détail les performances du modèle choisi et **justifiez** les."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "k_jCDj02XG1v",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0ca0dde0-75b1-4e11-901e-ffe3e1c0d024"
      },
      "source": [
        "# Train selected model\n",
        "selected_model = models[0]\n",
        "selected_model.fit(X_train_preprocess, y_train_label)\n",
        "y_pred = selected_model.predict(X_train_preprocess)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0:\tlearn: 0.6818218\ttotal: 45.1ms\tremaining: 45s\n",
            "1:\tlearn: 0.6700337\ttotal: 88.3ms\tremaining: 44.1s\n",
            "2:\tlearn: 0.6584603\ttotal: 147ms\tremaining: 48.7s\n",
            "3:\tlearn: 0.6488723\ttotal: 190ms\tremaining: 47.4s\n",
            "4:\tlearn: 0.6384057\ttotal: 233ms\tremaining: 46.3s\n",
            "5:\tlearn: 0.6281412\ttotal: 282ms\tremaining: 46.7s\n",
            "6:\tlearn: 0.6180828\ttotal: 327ms\tremaining: 46.3s\n",
            "7:\tlearn: 0.6099697\ttotal: 369ms\tremaining: 45.7s\n",
            "8:\tlearn: 0.6009164\ttotal: 412ms\tremaining: 45.4s\n",
            "9:\tlearn: 0.5921597\ttotal: 455ms\tremaining: 45s\n",
            "10:\tlearn: 0.5840902\ttotal: 500ms\tremaining: 45s\n",
            "11:\tlearn: 0.5760158\ttotal: 546ms\tremaining: 44.9s\n",
            "12:\tlearn: 0.5678481\ttotal: 591ms\tremaining: 44.9s\n",
            "13:\tlearn: 0.5597566\ttotal: 634ms\tremaining: 44.6s\n",
            "14:\tlearn: 0.5522181\ttotal: 679ms\tremaining: 44.6s\n",
            "15:\tlearn: 0.5452683\ttotal: 724ms\tremaining: 44.5s\n",
            "16:\tlearn: 0.5390277\ttotal: 748ms\tremaining: 43.3s\n",
            "17:\tlearn: 0.5316099\ttotal: 794ms\tremaining: 43.3s\n",
            "18:\tlearn: 0.5253468\ttotal: 838ms\tremaining: 43.2s\n",
            "19:\tlearn: 0.5185522\ttotal: 885ms\tremaining: 43.4s\n",
            "20:\tlearn: 0.5123042\ttotal: 932ms\tremaining: 43.4s\n",
            "21:\tlearn: 0.5062966\ttotal: 976ms\tremaining: 43.4s\n",
            "22:\tlearn: 0.5003059\ttotal: 1.02s\tremaining: 43.4s\n",
            "23:\tlearn: 0.4946190\ttotal: 1.06s\tremaining: 43.3s\n",
            "24:\tlearn: 0.4894341\ttotal: 1.11s\tremaining: 43.3s\n",
            "25:\tlearn: 0.4837400\ttotal: 1.16s\tremaining: 43.4s\n",
            "26:\tlearn: 0.4784896\ttotal: 1.2s\tremaining: 43.2s\n",
            "27:\tlearn: 0.4737306\ttotal: 1.24s\tremaining: 43.1s\n",
            "28:\tlearn: 0.4691656\ttotal: 1.28s\tremaining: 42.9s\n",
            "29:\tlearn: 0.4643310\ttotal: 1.32s\tremaining: 42.8s\n",
            "30:\tlearn: 0.4598708\ttotal: 1.37s\tremaining: 42.8s\n",
            "31:\tlearn: 0.4555449\ttotal: 1.41s\tremaining: 42.7s\n",
            "32:\tlearn: 0.4513939\ttotal: 1.45s\tremaining: 42.6s\n",
            "33:\tlearn: 0.4472707\ttotal: 1.5s\tremaining: 42.5s\n",
            "34:\tlearn: 0.4431806\ttotal: 1.54s\tremaining: 42.4s\n",
            "35:\tlearn: 0.4392299\ttotal: 1.58s\tremaining: 42.4s\n",
            "36:\tlearn: 0.4356975\ttotal: 1.63s\tremaining: 42.4s\n",
            "37:\tlearn: 0.4319848\ttotal: 1.68s\tremaining: 42.4s\n",
            "38:\tlearn: 0.4285875\ttotal: 1.72s\tremaining: 42.4s\n",
            "39:\tlearn: 0.4251626\ttotal: 1.76s\tremaining: 42.3s\n",
            "40:\tlearn: 0.4221302\ttotal: 1.81s\tremaining: 42.4s\n",
            "41:\tlearn: 0.4191061\ttotal: 1.85s\tremaining: 42.3s\n",
            "42:\tlearn: 0.4160073\ttotal: 1.9s\tremaining: 42.3s\n",
            "43:\tlearn: 0.4130240\ttotal: 1.95s\tremaining: 42.3s\n",
            "44:\tlearn: 0.4102471\ttotal: 1.99s\tremaining: 42.2s\n",
            "45:\tlearn: 0.4073626\ttotal: 2.04s\tremaining: 42.2s\n",
            "46:\tlearn: 0.4047732\ttotal: 2.08s\tremaining: 42.2s\n",
            "47:\tlearn: 0.4021636\ttotal: 2.12s\tremaining: 42.1s\n",
            "48:\tlearn: 0.3997434\ttotal: 2.16s\tremaining: 42s\n",
            "49:\tlearn: 0.3972769\ttotal: 2.21s\tremaining: 41.9s\n",
            "50:\tlearn: 0.3949282\ttotal: 2.25s\tremaining: 42s\n",
            "51:\tlearn: 0.3928385\ttotal: 2.3s\tremaining: 41.9s\n",
            "52:\tlearn: 0.3906266\ttotal: 2.34s\tremaining: 41.8s\n",
            "53:\tlearn: 0.3885137\ttotal: 2.38s\tremaining: 41.8s\n",
            "54:\tlearn: 0.3866450\ttotal: 2.43s\tremaining: 41.7s\n",
            "55:\tlearn: 0.3848152\ttotal: 2.48s\tremaining: 41.8s\n",
            "56:\tlearn: 0.3828639\ttotal: 2.52s\tremaining: 41.7s\n",
            "57:\tlearn: 0.3808304\ttotal: 2.56s\tremaining: 41.7s\n",
            "58:\tlearn: 0.3790608\ttotal: 2.61s\tremaining: 41.6s\n",
            "59:\tlearn: 0.3773433\ttotal: 2.66s\tremaining: 41.6s\n",
            "60:\tlearn: 0.3754914\ttotal: 2.71s\tremaining: 41.6s\n",
            "61:\tlearn: 0.3737473\ttotal: 2.75s\tremaining: 41.6s\n",
            "62:\tlearn: 0.3720063\ttotal: 2.79s\tremaining: 41.5s\n",
            "63:\tlearn: 0.3703586\ttotal: 2.84s\tremaining: 41.5s\n",
            "64:\tlearn: 0.3687440\ttotal: 2.88s\tremaining: 41.4s\n",
            "65:\tlearn: 0.3670500\ttotal: 2.93s\tremaining: 41.4s\n",
            "66:\tlearn: 0.3654908\ttotal: 2.97s\tremaining: 41.4s\n",
            "67:\tlearn: 0.3639303\ttotal: 3.01s\tremaining: 41.3s\n",
            "68:\tlearn: 0.3624708\ttotal: 3.06s\tremaining: 41.3s\n",
            "69:\tlearn: 0.3610340\ttotal: 3.1s\tremaining: 41.2s\n",
            "70:\tlearn: 0.3597339\ttotal: 3.15s\tremaining: 41.2s\n",
            "71:\tlearn: 0.3583701\ttotal: 3.19s\tremaining: 41.1s\n",
            "72:\tlearn: 0.3570697\ttotal: 3.23s\tremaining: 41.1s\n",
            "73:\tlearn: 0.3559075\ttotal: 3.28s\tremaining: 41s\n",
            "74:\tlearn: 0.3546904\ttotal: 3.32s\tremaining: 41s\n",
            "75:\tlearn: 0.3535479\ttotal: 3.37s\tremaining: 40.9s\n",
            "76:\tlearn: 0.3523839\ttotal: 3.41s\tremaining: 40.9s\n",
            "77:\tlearn: 0.3513541\ttotal: 3.46s\tremaining: 40.9s\n",
            "78:\tlearn: 0.3501976\ttotal: 3.51s\tremaining: 40.9s\n",
            "79:\tlearn: 0.3494935\ttotal: 3.53s\tremaining: 40.6s\n",
            "80:\tlearn: 0.3483834\ttotal: 3.58s\tremaining: 40.6s\n",
            "81:\tlearn: 0.3472802\ttotal: 3.62s\tremaining: 40.5s\n",
            "82:\tlearn: 0.3461608\ttotal: 3.66s\tremaining: 40.5s\n",
            "83:\tlearn: 0.3451859\ttotal: 3.71s\tremaining: 40.4s\n",
            "84:\tlearn: 0.3441423\ttotal: 3.75s\tremaining: 40.4s\n",
            "85:\tlearn: 0.3432144\ttotal: 3.8s\tremaining: 40.3s\n",
            "86:\tlearn: 0.3423510\ttotal: 3.84s\tremaining: 40.3s\n",
            "87:\tlearn: 0.3415171\ttotal: 3.88s\tremaining: 40.2s\n",
            "88:\tlearn: 0.3406739\ttotal: 3.92s\tremaining: 40.2s\n",
            "89:\tlearn: 0.3399066\ttotal: 3.97s\tremaining: 40.1s\n",
            "90:\tlearn: 0.3390370\ttotal: 4.01s\tremaining: 40.1s\n",
            "91:\tlearn: 0.3383125\ttotal: 4.06s\tremaining: 40.1s\n",
            "92:\tlearn: 0.3375590\ttotal: 4.1s\tremaining: 40s\n",
            "93:\tlearn: 0.3369072\ttotal: 4.15s\tremaining: 40s\n",
            "94:\tlearn: 0.3361295\ttotal: 4.19s\tremaining: 39.9s\n",
            "95:\tlearn: 0.3353947\ttotal: 4.24s\tremaining: 39.9s\n",
            "96:\tlearn: 0.3345566\ttotal: 4.28s\tremaining: 39.9s\n",
            "97:\tlearn: 0.3338112\ttotal: 4.32s\tremaining: 39.8s\n",
            "98:\tlearn: 0.3332127\ttotal: 4.37s\tremaining: 39.8s\n",
            "99:\tlearn: 0.3326110\ttotal: 4.41s\tremaining: 39.7s\n",
            "100:\tlearn: 0.3318913\ttotal: 4.46s\tremaining: 39.7s\n",
            "101:\tlearn: 0.3312857\ttotal: 4.5s\tremaining: 39.6s\n",
            "102:\tlearn: 0.3306377\ttotal: 4.54s\tremaining: 39.6s\n",
            "103:\tlearn: 0.3302136\ttotal: 4.59s\tremaining: 39.5s\n",
            "104:\tlearn: 0.3295765\ttotal: 4.63s\tremaining: 39.5s\n",
            "105:\tlearn: 0.3289330\ttotal: 4.68s\tremaining: 39.4s\n",
            "106:\tlearn: 0.3283051\ttotal: 4.72s\tremaining: 39.4s\n",
            "107:\tlearn: 0.3277611\ttotal: 4.76s\tremaining: 39.4s\n",
            "108:\tlearn: 0.3272617\ttotal: 4.81s\tremaining: 39.3s\n",
            "109:\tlearn: 0.3267248\ttotal: 4.85s\tremaining: 39.3s\n",
            "110:\tlearn: 0.3262951\ttotal: 4.9s\tremaining: 39.2s\n",
            "111:\tlearn: 0.3256788\ttotal: 4.95s\tremaining: 39.2s\n",
            "112:\tlearn: 0.3250806\ttotal: 4.99s\tremaining: 39.2s\n",
            "113:\tlearn: 0.3245910\ttotal: 5.04s\tremaining: 39.1s\n",
            "114:\tlearn: 0.3240921\ttotal: 5.08s\tremaining: 39.1s\n",
            "115:\tlearn: 0.3235967\ttotal: 5.12s\tremaining: 39.1s\n",
            "116:\tlearn: 0.3231224\ttotal: 5.17s\tremaining: 39s\n",
            "117:\tlearn: 0.3226414\ttotal: 5.21s\tremaining: 39s\n",
            "118:\tlearn: 0.3221292\ttotal: 5.26s\tremaining: 38.9s\n",
            "119:\tlearn: 0.3216876\ttotal: 5.3s\tremaining: 38.9s\n",
            "120:\tlearn: 0.3213099\ttotal: 5.35s\tremaining: 38.8s\n",
            "121:\tlearn: 0.3208980\ttotal: 5.39s\tremaining: 38.8s\n",
            "122:\tlearn: 0.3204167\ttotal: 5.43s\tremaining: 38.7s\n",
            "123:\tlearn: 0.3199629\ttotal: 5.47s\tremaining: 38.7s\n",
            "124:\tlearn: 0.3194928\ttotal: 5.52s\tremaining: 38.6s\n",
            "125:\tlearn: 0.3190330\ttotal: 5.56s\tremaining: 38.6s\n",
            "126:\tlearn: 0.3186601\ttotal: 5.61s\tremaining: 38.5s\n",
            "127:\tlearn: 0.3181198\ttotal: 5.65s\tremaining: 38.5s\n",
            "128:\tlearn: 0.3176974\ttotal: 5.69s\tremaining: 38.4s\n",
            "129:\tlearn: 0.3172588\ttotal: 5.74s\tremaining: 38.4s\n",
            "130:\tlearn: 0.3167975\ttotal: 5.79s\tremaining: 38.4s\n",
            "131:\tlearn: 0.3165140\ttotal: 5.83s\tremaining: 38.3s\n",
            "132:\tlearn: 0.3162180\ttotal: 5.87s\tremaining: 38.3s\n",
            "133:\tlearn: 0.3158404\ttotal: 5.92s\tremaining: 38.2s\n",
            "134:\tlearn: 0.3154126\ttotal: 5.96s\tremaining: 38.2s\n",
            "135:\tlearn: 0.3150505\ttotal: 6.01s\tremaining: 38.2s\n",
            "136:\tlearn: 0.3147088\ttotal: 6.05s\tremaining: 38.1s\n",
            "137:\tlearn: 0.3143940\ttotal: 6.1s\tremaining: 38.1s\n",
            "138:\tlearn: 0.3139888\ttotal: 6.14s\tremaining: 38s\n",
            "139:\tlearn: 0.3136734\ttotal: 6.18s\tremaining: 38s\n",
            "140:\tlearn: 0.3133030\ttotal: 6.23s\tremaining: 38s\n",
            "141:\tlearn: 0.3129479\ttotal: 6.27s\tremaining: 37.9s\n",
            "142:\tlearn: 0.3125932\ttotal: 6.32s\tremaining: 37.8s\n",
            "143:\tlearn: 0.3123078\ttotal: 6.36s\tremaining: 37.8s\n",
            "144:\tlearn: 0.3120393\ttotal: 6.4s\tremaining: 37.8s\n",
            "145:\tlearn: 0.3117242\ttotal: 6.45s\tremaining: 37.7s\n",
            "146:\tlearn: 0.3113552\ttotal: 6.49s\tremaining: 37.7s\n",
            "147:\tlearn: 0.3110558\ttotal: 6.53s\tremaining: 37.6s\n",
            "148:\tlearn: 0.3108324\ttotal: 6.58s\tremaining: 37.6s\n",
            "149:\tlearn: 0.3105494\ttotal: 6.62s\tremaining: 37.5s\n",
            "150:\tlearn: 0.3102314\ttotal: 6.67s\tremaining: 37.5s\n",
            "151:\tlearn: 0.3099963\ttotal: 6.71s\tremaining: 37.4s\n",
            "152:\tlearn: 0.3096561\ttotal: 6.75s\tremaining: 37.4s\n",
            "153:\tlearn: 0.3093971\ttotal: 6.8s\tremaining: 37.3s\n",
            "154:\tlearn: 0.3091948\ttotal: 6.84s\tremaining: 37.3s\n",
            "155:\tlearn: 0.3089596\ttotal: 6.89s\tremaining: 37.3s\n",
            "156:\tlearn: 0.3087246\ttotal: 6.93s\tremaining: 37.2s\n",
            "157:\tlearn: 0.3083831\ttotal: 6.98s\tremaining: 37.2s\n",
            "158:\tlearn: 0.3081054\ttotal: 7.02s\tremaining: 37.1s\n",
            "159:\tlearn: 0.3078523\ttotal: 7.07s\tremaining: 37.1s\n",
            "160:\tlearn: 0.3076161\ttotal: 7.11s\tremaining: 37.1s\n",
            "161:\tlearn: 0.3074171\ttotal: 7.16s\tremaining: 37s\n",
            "162:\tlearn: 0.3071741\ttotal: 7.2s\tremaining: 37s\n",
            "163:\tlearn: 0.3068250\ttotal: 7.24s\tremaining: 36.9s\n",
            "164:\tlearn: 0.3065645\ttotal: 7.29s\tremaining: 36.9s\n",
            "165:\tlearn: 0.3063375\ttotal: 7.34s\tremaining: 36.9s\n",
            "166:\tlearn: 0.3059788\ttotal: 7.38s\tremaining: 36.8s\n",
            "167:\tlearn: 0.3057565\ttotal: 7.42s\tremaining: 36.8s\n",
            "168:\tlearn: 0.3055410\ttotal: 7.47s\tremaining: 36.7s\n",
            "169:\tlearn: 0.3052648\ttotal: 7.51s\tremaining: 36.7s\n",
            "170:\tlearn: 0.3049838\ttotal: 7.56s\tremaining: 36.6s\n",
            "171:\tlearn: 0.3046898\ttotal: 7.6s\tremaining: 36.6s\n",
            "172:\tlearn: 0.3044374\ttotal: 7.65s\tremaining: 36.6s\n",
            "173:\tlearn: 0.3042563\ttotal: 7.69s\tremaining: 36.5s\n",
            "174:\tlearn: 0.3040356\ttotal: 7.73s\tremaining: 36.5s\n",
            "175:\tlearn: 0.3038077\ttotal: 7.78s\tremaining: 36.4s\n",
            "176:\tlearn: 0.3035426\ttotal: 7.82s\tremaining: 36.4s\n",
            "177:\tlearn: 0.3033033\ttotal: 7.87s\tremaining: 36.3s\n",
            "178:\tlearn: 0.3030946\ttotal: 7.91s\tremaining: 36.3s\n",
            "179:\tlearn: 0.3029136\ttotal: 7.97s\tremaining: 36.3s\n",
            "180:\tlearn: 0.3026943\ttotal: 8.02s\tremaining: 36.3s\n",
            "181:\tlearn: 0.3024659\ttotal: 8.07s\tremaining: 36.3s\n",
            "182:\tlearn: 0.3021366\ttotal: 8.11s\tremaining: 36.2s\n",
            "183:\tlearn: 0.3019227\ttotal: 8.16s\tremaining: 36.2s\n",
            "184:\tlearn: 0.3017278\ttotal: 8.2s\tremaining: 36.1s\n",
            "185:\tlearn: 0.3016173\ttotal: 8.23s\tremaining: 36s\n",
            "186:\tlearn: 0.3014679\ttotal: 8.28s\tremaining: 36s\n",
            "187:\tlearn: 0.3012937\ttotal: 8.32s\tremaining: 35.9s\n",
            "188:\tlearn: 0.3010448\ttotal: 8.37s\tremaining: 35.9s\n",
            "189:\tlearn: 0.3007977\ttotal: 8.41s\tremaining: 35.8s\n",
            "190:\tlearn: 0.3006159\ttotal: 8.45s\tremaining: 35.8s\n",
            "191:\tlearn: 0.3004339\ttotal: 8.5s\tremaining: 35.8s\n",
            "192:\tlearn: 0.3002339\ttotal: 8.54s\tremaining: 35.7s\n",
            "193:\tlearn: 0.3000748\ttotal: 8.59s\tremaining: 35.7s\n",
            "194:\tlearn: 0.2998857\ttotal: 8.63s\tremaining: 35.6s\n",
            "195:\tlearn: 0.2996513\ttotal: 8.68s\tremaining: 35.6s\n",
            "196:\tlearn: 0.2994668\ttotal: 8.72s\tremaining: 35.5s\n",
            "197:\tlearn: 0.2992305\ttotal: 8.76s\tremaining: 35.5s\n",
            "198:\tlearn: 0.2990945\ttotal: 8.8s\tremaining: 35.4s\n",
            "199:\tlearn: 0.2989271\ttotal: 8.85s\tremaining: 35.4s\n",
            "200:\tlearn: 0.2987603\ttotal: 8.9s\tremaining: 35.4s\n",
            "201:\tlearn: 0.2985430\ttotal: 8.94s\tremaining: 35.3s\n",
            "202:\tlearn: 0.2984009\ttotal: 8.98s\tremaining: 35.3s\n",
            "203:\tlearn: 0.2982316\ttotal: 9.03s\tremaining: 35.2s\n",
            "204:\tlearn: 0.2979904\ttotal: 9.08s\tremaining: 35.2s\n",
            "205:\tlearn: 0.2978323\ttotal: 9.13s\tremaining: 35.2s\n",
            "206:\tlearn: 0.2976465\ttotal: 9.18s\tremaining: 35.2s\n",
            "207:\tlearn: 0.2974250\ttotal: 9.22s\tremaining: 35.1s\n",
            "208:\tlearn: 0.2973197\ttotal: 9.26s\tremaining: 35.1s\n",
            "209:\tlearn: 0.2971794\ttotal: 9.31s\tremaining: 35s\n",
            "210:\tlearn: 0.2970197\ttotal: 9.36s\tremaining: 35s\n",
            "211:\tlearn: 0.2968940\ttotal: 9.4s\tremaining: 34.9s\n",
            "212:\tlearn: 0.2967509\ttotal: 9.44s\tremaining: 34.9s\n",
            "213:\tlearn: 0.2966222\ttotal: 9.49s\tremaining: 34.8s\n",
            "214:\tlearn: 0.2964501\ttotal: 9.53s\tremaining: 34.8s\n",
            "215:\tlearn: 0.2963040\ttotal: 9.58s\tremaining: 34.8s\n",
            "216:\tlearn: 0.2960857\ttotal: 9.62s\tremaining: 34.7s\n",
            "217:\tlearn: 0.2959461\ttotal: 9.67s\tremaining: 34.7s\n",
            "218:\tlearn: 0.2957487\ttotal: 9.71s\tremaining: 34.6s\n",
            "219:\tlearn: 0.2955999\ttotal: 9.76s\tremaining: 34.6s\n",
            "220:\tlearn: 0.2954819\ttotal: 9.8s\tremaining: 34.6s\n",
            "221:\tlearn: 0.2952782\ttotal: 9.84s\tremaining: 34.5s\n",
            "222:\tlearn: 0.2950975\ttotal: 9.89s\tremaining: 34.5s\n",
            "223:\tlearn: 0.2949279\ttotal: 9.94s\tremaining: 34.4s\n",
            "224:\tlearn: 0.2947826\ttotal: 9.98s\tremaining: 34.4s\n",
            "225:\tlearn: 0.2946785\ttotal: 10s\tremaining: 34.3s\n",
            "226:\tlearn: 0.2944794\ttotal: 10.1s\tremaining: 34.3s\n",
            "227:\tlearn: 0.2942893\ttotal: 10.1s\tremaining: 34.2s\n",
            "228:\tlearn: 0.2941605\ttotal: 10.2s\tremaining: 34.2s\n",
            "229:\tlearn: 0.2940454\ttotal: 10.2s\tremaining: 34.1s\n",
            "230:\tlearn: 0.2939341\ttotal: 10.3s\tremaining: 34.1s\n",
            "231:\tlearn: 0.2938008\ttotal: 10.3s\tremaining: 34.1s\n",
            "232:\tlearn: 0.2936395\ttotal: 10.3s\tremaining: 34s\n",
            "233:\tlearn: 0.2935093\ttotal: 10.4s\tremaining: 34s\n",
            "234:\tlearn: 0.2933899\ttotal: 10.4s\tremaining: 33.9s\n",
            "235:\tlearn: 0.2931423\ttotal: 10.5s\tremaining: 33.9s\n",
            "236:\tlearn: 0.2930189\ttotal: 10.5s\tremaining: 33.9s\n",
            "237:\tlearn: 0.2928568\ttotal: 10.6s\tremaining: 33.8s\n",
            "238:\tlearn: 0.2927253\ttotal: 10.6s\tremaining: 33.8s\n",
            "239:\tlearn: 0.2926259\ttotal: 10.6s\tremaining: 33.7s\n",
            "240:\tlearn: 0.2925196\ttotal: 10.7s\tremaining: 33.7s\n",
            "241:\tlearn: 0.2923893\ttotal: 10.7s\tremaining: 33.6s\n",
            "242:\tlearn: 0.2922687\ttotal: 10.8s\tremaining: 33.6s\n",
            "243:\tlearn: 0.2921722\ttotal: 10.8s\tremaining: 33.5s\n",
            "244:\tlearn: 0.2920423\ttotal: 10.9s\tremaining: 33.5s\n",
            "245:\tlearn: 0.2919546\ttotal: 10.9s\tremaining: 33.5s\n",
            "246:\tlearn: 0.2918282\ttotal: 11s\tremaining: 33.4s\n",
            "247:\tlearn: 0.2916529\ttotal: 11s\tremaining: 33.4s\n",
            "248:\tlearn: 0.2915321\ttotal: 11s\tremaining: 33.3s\n",
            "249:\tlearn: 0.2914187\ttotal: 11.1s\tremaining: 33.3s\n",
            "250:\tlearn: 0.2912799\ttotal: 11.1s\tremaining: 33.2s\n",
            "251:\tlearn: 0.2911983\ttotal: 11.2s\tremaining: 33.2s\n",
            "252:\tlearn: 0.2910531\ttotal: 11.2s\tremaining: 33.1s\n",
            "253:\tlearn: 0.2909347\ttotal: 11.3s\tremaining: 33.1s\n",
            "254:\tlearn: 0.2907884\ttotal: 11.3s\tremaining: 33s\n",
            "255:\tlearn: 0.2906318\ttotal: 11.4s\tremaining: 33s\n",
            "256:\tlearn: 0.2904534\ttotal: 11.4s\tremaining: 33s\n",
            "257:\tlearn: 0.2903066\ttotal: 11.4s\tremaining: 32.9s\n",
            "258:\tlearn: 0.2901963\ttotal: 11.5s\tremaining: 32.9s\n",
            "259:\tlearn: 0.2900885\ttotal: 11.5s\tremaining: 32.8s\n",
            "260:\tlearn: 0.2899748\ttotal: 11.6s\tremaining: 32.8s\n",
            "261:\tlearn: 0.2898760\ttotal: 11.6s\tremaining: 32.7s\n",
            "262:\tlearn: 0.2897755\ttotal: 11.7s\tremaining: 32.7s\n",
            "263:\tlearn: 0.2896912\ttotal: 11.7s\tremaining: 32.6s\n",
            "264:\tlearn: 0.2895521\ttotal: 11.8s\tremaining: 32.6s\n",
            "265:\tlearn: 0.2893903\ttotal: 11.8s\tremaining: 32.6s\n",
            "266:\tlearn: 0.2892689\ttotal: 11.8s\tremaining: 32.5s\n",
            "267:\tlearn: 0.2891126\ttotal: 11.9s\tremaining: 32.5s\n",
            "268:\tlearn: 0.2890077\ttotal: 11.9s\tremaining: 32.4s\n",
            "269:\tlearn: 0.2888867\ttotal: 12s\tremaining: 32.4s\n",
            "270:\tlearn: 0.2887704\ttotal: 12s\tremaining: 32.3s\n",
            "271:\tlearn: 0.2886593\ttotal: 12.1s\tremaining: 32.3s\n",
            "272:\tlearn: 0.2885134\ttotal: 12.1s\tremaining: 32.2s\n",
            "273:\tlearn: 0.2883364\ttotal: 12.1s\tremaining: 32.2s\n",
            "274:\tlearn: 0.2882513\ttotal: 12.2s\tremaining: 32.2s\n",
            "275:\tlearn: 0.2882430\ttotal: 12.2s\tremaining: 32s\n",
            "276:\tlearn: 0.2881451\ttotal: 12.3s\tremaining: 32s\n",
            "277:\tlearn: 0.2880142\ttotal: 12.3s\tremaining: 31.9s\n",
            "278:\tlearn: 0.2878780\ttotal: 12.3s\tremaining: 31.9s\n",
            "279:\tlearn: 0.2877900\ttotal: 12.4s\tremaining: 31.9s\n",
            "280:\tlearn: 0.2877008\ttotal: 12.4s\tremaining: 31.8s\n",
            "281:\tlearn: 0.2875929\ttotal: 12.5s\tremaining: 31.8s\n",
            "282:\tlearn: 0.2874675\ttotal: 12.5s\tremaining: 31.7s\n",
            "283:\tlearn: 0.2873683\ttotal: 12.6s\tremaining: 31.7s\n",
            "284:\tlearn: 0.2873027\ttotal: 12.6s\tremaining: 31.7s\n",
            "285:\tlearn: 0.2872036\ttotal: 12.7s\tremaining: 31.6s\n",
            "286:\tlearn: 0.2871203\ttotal: 12.7s\tremaining: 31.6s\n",
            "287:\tlearn: 0.2869781\ttotal: 12.8s\tremaining: 31.5s\n",
            "288:\tlearn: 0.2868836\ttotal: 12.8s\tremaining: 31.5s\n",
            "289:\tlearn: 0.2867454\ttotal: 12.8s\tremaining: 31.4s\n",
            "290:\tlearn: 0.2866210\ttotal: 12.9s\tremaining: 31.4s\n",
            "291:\tlearn: 0.2864865\ttotal: 12.9s\tremaining: 31.4s\n",
            "292:\tlearn: 0.2864023\ttotal: 13s\tremaining: 31.3s\n",
            "293:\tlearn: 0.2862756\ttotal: 13s\tremaining: 31.3s\n",
            "294:\tlearn: 0.2861858\ttotal: 13.1s\tremaining: 31.2s\n",
            "295:\tlearn: 0.2860923\ttotal: 13.1s\tremaining: 31.2s\n",
            "296:\tlearn: 0.2859321\ttotal: 13.2s\tremaining: 31.1s\n",
            "297:\tlearn: 0.2858230\ttotal: 13.2s\tremaining: 31.1s\n",
            "298:\tlearn: 0.2856799\ttotal: 13.2s\tremaining: 31.1s\n",
            "299:\tlearn: 0.2855939\ttotal: 13.3s\tremaining: 31s\n",
            "300:\tlearn: 0.2854967\ttotal: 13.3s\tremaining: 31s\n",
            "301:\tlearn: 0.2853983\ttotal: 13.4s\tremaining: 30.9s\n",
            "302:\tlearn: 0.2852402\ttotal: 13.4s\tremaining: 30.9s\n",
            "303:\tlearn: 0.2851542\ttotal: 13.5s\tremaining: 30.8s\n",
            "304:\tlearn: 0.2850615\ttotal: 13.5s\tremaining: 30.8s\n",
            "305:\tlearn: 0.2848727\ttotal: 13.6s\tremaining: 30.7s\n",
            "306:\tlearn: 0.2847793\ttotal: 13.6s\tremaining: 30.7s\n",
            "307:\tlearn: 0.2846870\ttotal: 13.6s\tremaining: 30.7s\n",
            "308:\tlearn: 0.2845968\ttotal: 13.7s\tremaining: 30.6s\n",
            "309:\tlearn: 0.2844566\ttotal: 13.7s\tremaining: 30.6s\n",
            "310:\tlearn: 0.2843626\ttotal: 13.8s\tremaining: 30.5s\n",
            "311:\tlearn: 0.2841704\ttotal: 13.8s\tremaining: 30.5s\n",
            "312:\tlearn: 0.2840762\ttotal: 13.9s\tremaining: 30.5s\n",
            "313:\tlearn: 0.2839980\ttotal: 13.9s\tremaining: 30.4s\n",
            "314:\tlearn: 0.2838808\ttotal: 14s\tremaining: 30.4s\n",
            "315:\tlearn: 0.2837674\ttotal: 14s\tremaining: 30.3s\n",
            "316:\tlearn: 0.2836814\ttotal: 14.1s\tremaining: 30.3s\n",
            "317:\tlearn: 0.2835748\ttotal: 14.1s\tremaining: 30.2s\n",
            "318:\tlearn: 0.2834707\ttotal: 14.1s\tremaining: 30.2s\n",
            "319:\tlearn: 0.2833746\ttotal: 14.2s\tremaining: 30.1s\n",
            "320:\tlearn: 0.2832453\ttotal: 14.2s\tremaining: 30.1s\n",
            "321:\tlearn: 0.2831732\ttotal: 14.3s\tremaining: 30.1s\n",
            "322:\tlearn: 0.2830672\ttotal: 14.3s\tremaining: 30s\n",
            "323:\tlearn: 0.2829934\ttotal: 14.4s\tremaining: 30s\n",
            "324:\tlearn: 0.2829019\ttotal: 14.4s\tremaining: 29.9s\n",
            "325:\tlearn: 0.2828015\ttotal: 14.4s\tremaining: 29.9s\n",
            "326:\tlearn: 0.2827136\ttotal: 14.5s\tremaining: 29.8s\n",
            "327:\tlearn: 0.2826177\ttotal: 14.5s\tremaining: 29.8s\n",
            "328:\tlearn: 0.2825325\ttotal: 14.6s\tremaining: 29.8s\n",
            "329:\tlearn: 0.2824463\ttotal: 14.6s\tremaining: 29.7s\n",
            "330:\tlearn: 0.2823371\ttotal: 14.7s\tremaining: 29.7s\n",
            "331:\tlearn: 0.2822361\ttotal: 14.7s\tremaining: 29.6s\n",
            "332:\tlearn: 0.2821266\ttotal: 14.8s\tremaining: 29.6s\n",
            "333:\tlearn: 0.2820502\ttotal: 14.8s\tremaining: 29.5s\n",
            "334:\tlearn: 0.2819713\ttotal: 14.8s\tremaining: 29.5s\n",
            "335:\tlearn: 0.2819038\ttotal: 14.9s\tremaining: 29.4s\n",
            "336:\tlearn: 0.2818102\ttotal: 14.9s\tremaining: 29.4s\n",
            "337:\tlearn: 0.2817455\ttotal: 15s\tremaining: 29.3s\n",
            "338:\tlearn: 0.2816780\ttotal: 15s\tremaining: 29.3s\n",
            "339:\tlearn: 0.2816010\ttotal: 15.1s\tremaining: 29.3s\n",
            "340:\tlearn: 0.2815140\ttotal: 15.1s\tremaining: 29.2s\n",
            "341:\tlearn: 0.2813725\ttotal: 15.2s\tremaining: 29.2s\n",
            "342:\tlearn: 0.2812954\ttotal: 15.2s\tremaining: 29.1s\n",
            "343:\tlearn: 0.2812034\ttotal: 15.3s\tremaining: 29.1s\n",
            "344:\tlearn: 0.2810843\ttotal: 15.3s\tremaining: 29s\n",
            "345:\tlearn: 0.2810042\ttotal: 15.3s\tremaining: 29s\n",
            "346:\tlearn: 0.2808448\ttotal: 15.4s\tremaining: 29s\n",
            "347:\tlearn: 0.2807796\ttotal: 15.4s\tremaining: 28.9s\n",
            "348:\tlearn: 0.2806570\ttotal: 15.5s\tremaining: 28.9s\n",
            "349:\tlearn: 0.2805643\ttotal: 15.5s\tremaining: 28.8s\n",
            "350:\tlearn: 0.2804350\ttotal: 15.6s\tremaining: 28.8s\n",
            "351:\tlearn: 0.2803142\ttotal: 15.6s\tremaining: 28.7s\n",
            "352:\tlearn: 0.2802285\ttotal: 15.7s\tremaining: 28.7s\n",
            "353:\tlearn: 0.2801675\ttotal: 15.7s\tremaining: 28.6s\n",
            "354:\tlearn: 0.2800118\ttotal: 15.7s\tremaining: 28.6s\n",
            "355:\tlearn: 0.2799315\ttotal: 15.8s\tremaining: 28.5s\n",
            "356:\tlearn: 0.2798528\ttotal: 15.8s\tremaining: 28.5s\n",
            "357:\tlearn: 0.2797373\ttotal: 15.9s\tremaining: 28.5s\n",
            "358:\tlearn: 0.2796680\ttotal: 15.9s\tremaining: 28.4s\n",
            "359:\tlearn: 0.2795246\ttotal: 16s\tremaining: 28.4s\n",
            "360:\tlearn: 0.2794646\ttotal: 16s\tremaining: 28.3s\n",
            "361:\tlearn: 0.2794065\ttotal: 16.1s\tremaining: 28.3s\n",
            "362:\tlearn: 0.2793010\ttotal: 16.1s\tremaining: 28.2s\n",
            "363:\tlearn: 0.2792146\ttotal: 16.1s\tremaining: 28.2s\n",
            "364:\tlearn: 0.2791369\ttotal: 16.2s\tremaining: 28.1s\n",
            "365:\tlearn: 0.2790692\ttotal: 16.2s\tremaining: 28.1s\n",
            "366:\tlearn: 0.2789856\ttotal: 16.3s\tremaining: 28.1s\n",
            "367:\tlearn: 0.2788881\ttotal: 16.3s\tremaining: 28s\n",
            "368:\tlearn: 0.2788131\ttotal: 16.4s\tremaining: 28s\n",
            "369:\tlearn: 0.2787335\ttotal: 16.4s\tremaining: 27.9s\n",
            "370:\tlearn: 0.2787275\ttotal: 16.4s\tremaining: 27.8s\n",
            "371:\tlearn: 0.2786499\ttotal: 16.5s\tremaining: 27.8s\n",
            "372:\tlearn: 0.2785547\ttotal: 16.5s\tremaining: 27.7s\n",
            "373:\tlearn: 0.2784465\ttotal: 16.5s\tremaining: 27.7s\n",
            "374:\tlearn: 0.2783511\ttotal: 16.6s\tremaining: 27.6s\n",
            "375:\tlearn: 0.2782884\ttotal: 16.6s\tremaining: 27.6s\n",
            "376:\tlearn: 0.2781976\ttotal: 16.7s\tremaining: 27.6s\n",
            "377:\tlearn: 0.2781109\ttotal: 16.7s\tremaining: 27.5s\n",
            "378:\tlearn: 0.2780262\ttotal: 16.8s\tremaining: 27.5s\n",
            "379:\tlearn: 0.2779302\ttotal: 16.8s\tremaining: 27.4s\n",
            "380:\tlearn: 0.2778356\ttotal: 16.9s\tremaining: 27.4s\n",
            "381:\tlearn: 0.2777495\ttotal: 16.9s\tremaining: 27.3s\n",
            "382:\tlearn: 0.2776707\ttotal: 17s\tremaining: 27.3s\n",
            "383:\tlearn: 0.2775859\ttotal: 17s\tremaining: 27.3s\n",
            "384:\tlearn: 0.2775385\ttotal: 17s\tremaining: 27.2s\n",
            "385:\tlearn: 0.2774626\ttotal: 17.1s\tremaining: 27.2s\n",
            "386:\tlearn: 0.2773704\ttotal: 17.1s\tremaining: 27.1s\n",
            "387:\tlearn: 0.2773627\ttotal: 17.1s\tremaining: 27s\n",
            "388:\tlearn: 0.2772748\ttotal: 17.2s\tremaining: 27s\n",
            "389:\tlearn: 0.2771568\ttotal: 17.2s\tremaining: 26.9s\n",
            "390:\tlearn: 0.2770553\ttotal: 17.3s\tremaining: 26.9s\n",
            "391:\tlearn: 0.2769886\ttotal: 17.3s\tremaining: 26.9s\n",
            "392:\tlearn: 0.2769260\ttotal: 17.4s\tremaining: 26.8s\n",
            "393:\tlearn: 0.2768103\ttotal: 17.4s\tremaining: 26.8s\n",
            "394:\tlearn: 0.2767308\ttotal: 17.4s\tremaining: 26.7s\n",
            "395:\tlearn: 0.2766137\ttotal: 17.5s\tremaining: 26.7s\n",
            "396:\tlearn: 0.2765420\ttotal: 17.5s\tremaining: 26.6s\n",
            "397:\tlearn: 0.2764054\ttotal: 17.6s\tremaining: 26.6s\n",
            "398:\tlearn: 0.2763389\ttotal: 17.6s\tremaining: 26.5s\n",
            "399:\tlearn: 0.2762710\ttotal: 17.7s\tremaining: 26.5s\n",
            "400:\tlearn: 0.2761703\ttotal: 17.7s\tremaining: 26.5s\n",
            "401:\tlearn: 0.2760781\ttotal: 17.8s\tremaining: 26.4s\n",
            "402:\tlearn: 0.2759788\ttotal: 17.8s\tremaining: 26.4s\n",
            "403:\tlearn: 0.2758556\ttotal: 17.9s\tremaining: 26.3s\n",
            "404:\tlearn: 0.2757880\ttotal: 17.9s\tremaining: 26.3s\n",
            "405:\tlearn: 0.2757297\ttotal: 17.9s\tremaining: 26.2s\n",
            "406:\tlearn: 0.2756793\ttotal: 18s\tremaining: 26.2s\n",
            "407:\tlearn: 0.2755725\ttotal: 18s\tremaining: 26.2s\n",
            "408:\tlearn: 0.2755256\ttotal: 18.1s\tremaining: 26.1s\n",
            "409:\tlearn: 0.2754419\ttotal: 18.1s\tremaining: 26.1s\n",
            "410:\tlearn: 0.2753781\ttotal: 18.2s\tremaining: 26.1s\n",
            "411:\tlearn: 0.2753365\ttotal: 18.2s\tremaining: 26s\n",
            "412:\tlearn: 0.2752482\ttotal: 18.3s\tremaining: 26s\n",
            "413:\tlearn: 0.2751746\ttotal: 18.3s\tremaining: 26s\n",
            "414:\tlearn: 0.2750939\ttotal: 18.4s\tremaining: 25.9s\n",
            "415:\tlearn: 0.2750252\ttotal: 18.4s\tremaining: 25.9s\n",
            "416:\tlearn: 0.2749515\ttotal: 18.5s\tremaining: 25.8s\n",
            "417:\tlearn: 0.2748818\ttotal: 18.5s\tremaining: 25.8s\n",
            "418:\tlearn: 0.2748383\ttotal: 18.6s\tremaining: 25.7s\n",
            "419:\tlearn: 0.2747522\ttotal: 18.6s\tremaining: 25.7s\n",
            "420:\tlearn: 0.2746924\ttotal: 18.6s\tremaining: 25.6s\n",
            "421:\tlearn: 0.2746183\ttotal: 18.7s\tremaining: 25.6s\n",
            "422:\tlearn: 0.2745518\ttotal: 18.7s\tremaining: 25.6s\n",
            "423:\tlearn: 0.2744651\ttotal: 18.8s\tremaining: 25.5s\n",
            "424:\tlearn: 0.2743978\ttotal: 18.8s\tremaining: 25.5s\n",
            "425:\tlearn: 0.2742852\ttotal: 18.9s\tremaining: 25.4s\n",
            "426:\tlearn: 0.2742178\ttotal: 18.9s\tremaining: 25.4s\n",
            "427:\tlearn: 0.2741537\ttotal: 19s\tremaining: 25.3s\n",
            "428:\tlearn: 0.2740806\ttotal: 19s\tremaining: 25.3s\n",
            "429:\tlearn: 0.2740211\ttotal: 19s\tremaining: 25.2s\n",
            "430:\tlearn: 0.2739697\ttotal: 19.1s\tremaining: 25.2s\n",
            "431:\tlearn: 0.2738971\ttotal: 19.1s\tremaining: 25.2s\n",
            "432:\tlearn: 0.2738376\ttotal: 19.2s\tremaining: 25.1s\n",
            "433:\tlearn: 0.2737793\ttotal: 19.2s\tremaining: 25.1s\n",
            "434:\tlearn: 0.2737073\ttotal: 19.3s\tremaining: 25s\n",
            "435:\tlearn: 0.2735793\ttotal: 19.3s\tremaining: 25s\n",
            "436:\tlearn: 0.2734972\ttotal: 19.4s\tremaining: 24.9s\n",
            "437:\tlearn: 0.2734389\ttotal: 19.4s\tremaining: 24.9s\n",
            "438:\tlearn: 0.2733680\ttotal: 19.4s\tremaining: 24.9s\n",
            "439:\tlearn: 0.2732827\ttotal: 19.5s\tremaining: 24.8s\n",
            "440:\tlearn: 0.2731931\ttotal: 19.5s\tremaining: 24.8s\n",
            "441:\tlearn: 0.2730930\ttotal: 19.6s\tremaining: 24.7s\n",
            "442:\tlearn: 0.2730245\ttotal: 19.6s\tremaining: 24.7s\n",
            "443:\tlearn: 0.2729226\ttotal: 19.7s\tremaining: 24.6s\n",
            "444:\tlearn: 0.2728354\ttotal: 19.7s\tremaining: 24.6s\n",
            "445:\tlearn: 0.2727710\ttotal: 19.8s\tremaining: 24.5s\n",
            "446:\tlearn: 0.2727161\ttotal: 19.8s\tremaining: 24.5s\n",
            "447:\tlearn: 0.2726322\ttotal: 19.9s\tremaining: 24.5s\n",
            "448:\tlearn: 0.2725561\ttotal: 19.9s\tremaining: 24.4s\n",
            "449:\tlearn: 0.2725052\ttotal: 19.9s\tremaining: 24.4s\n",
            "450:\tlearn: 0.2724361\ttotal: 20s\tremaining: 24.3s\n",
            "451:\tlearn: 0.2723795\ttotal: 20s\tremaining: 24.3s\n",
            "452:\tlearn: 0.2723094\ttotal: 20.1s\tremaining: 24.2s\n",
            "453:\tlearn: 0.2722474\ttotal: 20.1s\tremaining: 24.2s\n",
            "454:\tlearn: 0.2721795\ttotal: 20.2s\tremaining: 24.2s\n",
            "455:\tlearn: 0.2720948\ttotal: 20.2s\tremaining: 24.1s\n",
            "456:\tlearn: 0.2720332\ttotal: 20.3s\tremaining: 24.1s\n",
            "457:\tlearn: 0.2719640\ttotal: 20.3s\tremaining: 24s\n",
            "458:\tlearn: 0.2718983\ttotal: 20.3s\tremaining: 24s\n",
            "459:\tlearn: 0.2718318\ttotal: 20.4s\tremaining: 23.9s\n",
            "460:\tlearn: 0.2717765\ttotal: 20.4s\tremaining: 23.9s\n",
            "461:\tlearn: 0.2717195\ttotal: 20.5s\tremaining: 23.8s\n",
            "462:\tlearn: 0.2716790\ttotal: 20.5s\tremaining: 23.8s\n",
            "463:\tlearn: 0.2716230\ttotal: 20.6s\tremaining: 23.8s\n",
            "464:\tlearn: 0.2715463\ttotal: 20.6s\tremaining: 23.7s\n",
            "465:\tlearn: 0.2714842\ttotal: 20.7s\tremaining: 23.7s\n",
            "466:\tlearn: 0.2714674\ttotal: 20.7s\tremaining: 23.6s\n",
            "467:\tlearn: 0.2713284\ttotal: 20.7s\tremaining: 23.6s\n",
            "468:\tlearn: 0.2712112\ttotal: 20.8s\tremaining: 23.5s\n",
            "469:\tlearn: 0.2711669\ttotal: 20.8s\tremaining: 23.5s\n",
            "470:\tlearn: 0.2711110\ttotal: 20.9s\tremaining: 23.4s\n",
            "471:\tlearn: 0.2710463\ttotal: 20.9s\tremaining: 23.4s\n",
            "472:\tlearn: 0.2709820\ttotal: 20.9s\tremaining: 23.3s\n",
            "473:\tlearn: 0.2709222\ttotal: 21s\tremaining: 23.3s\n",
            "474:\tlearn: 0.2708790\ttotal: 21s\tremaining: 23.3s\n",
            "475:\tlearn: 0.2708269\ttotal: 21.1s\tremaining: 23.2s\n",
            "476:\tlearn: 0.2707676\ttotal: 21.1s\tremaining: 23.2s\n",
            "477:\tlearn: 0.2707071\ttotal: 21.2s\tremaining: 23.1s\n",
            "478:\tlearn: 0.2706301\ttotal: 21.2s\tremaining: 23.1s\n",
            "479:\tlearn: 0.2705814\ttotal: 21.3s\tremaining: 23s\n",
            "480:\tlearn: 0.2705311\ttotal: 21.3s\tremaining: 23s\n",
            "481:\tlearn: 0.2704787\ttotal: 21.3s\tremaining: 22.9s\n",
            "482:\tlearn: 0.2704147\ttotal: 21.4s\tremaining: 22.9s\n",
            "483:\tlearn: 0.2703406\ttotal: 21.4s\tremaining: 22.9s\n",
            "484:\tlearn: 0.2702475\ttotal: 21.5s\tremaining: 22.8s\n",
            "485:\tlearn: 0.2701807\ttotal: 21.5s\tremaining: 22.8s\n",
            "486:\tlearn: 0.2701241\ttotal: 21.6s\tremaining: 22.7s\n",
            "487:\tlearn: 0.2700575\ttotal: 21.6s\tremaining: 22.7s\n",
            "488:\tlearn: 0.2699939\ttotal: 21.7s\tremaining: 22.6s\n",
            "489:\tlearn: 0.2699505\ttotal: 21.7s\tremaining: 22.6s\n",
            "490:\tlearn: 0.2698807\ttotal: 21.7s\tremaining: 22.5s\n",
            "491:\tlearn: 0.2697997\ttotal: 21.8s\tremaining: 22.5s\n",
            "492:\tlearn: 0.2697279\ttotal: 21.8s\tremaining: 22.5s\n",
            "493:\tlearn: 0.2696629\ttotal: 21.9s\tremaining: 22.4s\n",
            "494:\tlearn: 0.2695946\ttotal: 21.9s\tremaining: 22.4s\n",
            "495:\tlearn: 0.2695783\ttotal: 21.9s\tremaining: 22.3s\n",
            "496:\tlearn: 0.2695080\ttotal: 22s\tremaining: 22.3s\n",
            "497:\tlearn: 0.2694380\ttotal: 22s\tremaining: 22.2s\n",
            "498:\tlearn: 0.2693722\ttotal: 22.1s\tremaining: 22.2s\n",
            "499:\tlearn: 0.2692902\ttotal: 22.1s\tremaining: 22.1s\n",
            "500:\tlearn: 0.2692420\ttotal: 22.2s\tremaining: 22.1s\n",
            "501:\tlearn: 0.2691981\ttotal: 22.2s\tremaining: 22s\n",
            "502:\tlearn: 0.2691249\ttotal: 22.2s\tremaining: 22s\n",
            "503:\tlearn: 0.2690713\ttotal: 22.3s\tremaining: 21.9s\n",
            "504:\tlearn: 0.2690183\ttotal: 22.3s\tremaining: 21.9s\n",
            "505:\tlearn: 0.2689484\ttotal: 22.4s\tremaining: 21.8s\n",
            "506:\tlearn: 0.2688935\ttotal: 22.4s\tremaining: 21.8s\n",
            "507:\tlearn: 0.2688255\ttotal: 22.5s\tremaining: 21.8s\n",
            "508:\tlearn: 0.2687873\ttotal: 22.5s\tremaining: 21.7s\n",
            "509:\tlearn: 0.2686780\ttotal: 22.6s\tremaining: 21.7s\n",
            "510:\tlearn: 0.2685955\ttotal: 22.6s\tremaining: 21.6s\n",
            "511:\tlearn: 0.2684948\ttotal: 22.6s\tremaining: 21.6s\n",
            "512:\tlearn: 0.2684523\ttotal: 22.7s\tremaining: 21.5s\n",
            "513:\tlearn: 0.2683720\ttotal: 22.7s\tremaining: 21.5s\n",
            "514:\tlearn: 0.2683085\ttotal: 22.8s\tremaining: 21.5s\n",
            "515:\tlearn: 0.2682744\ttotal: 22.8s\tremaining: 21.4s\n",
            "516:\tlearn: 0.2682071\ttotal: 22.9s\tremaining: 21.4s\n",
            "517:\tlearn: 0.2681559\ttotal: 22.9s\tremaining: 21.3s\n",
            "518:\tlearn: 0.2681046\ttotal: 23s\tremaining: 21.3s\n",
            "519:\tlearn: 0.2680495\ttotal: 23s\tremaining: 21.2s\n",
            "520:\tlearn: 0.2679714\ttotal: 23s\tremaining: 21.2s\n",
            "521:\tlearn: 0.2679035\ttotal: 23.1s\tremaining: 21.1s\n",
            "522:\tlearn: 0.2678327\ttotal: 23.1s\tremaining: 21.1s\n",
            "523:\tlearn: 0.2677787\ttotal: 23.2s\tremaining: 21.1s\n",
            "524:\tlearn: 0.2677351\ttotal: 23.2s\tremaining: 21s\n",
            "525:\tlearn: 0.2676585\ttotal: 23.3s\tremaining: 21s\n",
            "526:\tlearn: 0.2676019\ttotal: 23.3s\tremaining: 20.9s\n",
            "527:\tlearn: 0.2675528\ttotal: 23.4s\tremaining: 20.9s\n",
            "528:\tlearn: 0.2674911\ttotal: 23.4s\tremaining: 20.8s\n",
            "529:\tlearn: 0.2674294\ttotal: 23.5s\tremaining: 20.8s\n",
            "530:\tlearn: 0.2673730\ttotal: 23.5s\tremaining: 20.8s\n",
            "531:\tlearn: 0.2673354\ttotal: 23.6s\tremaining: 20.7s\n",
            "532:\tlearn: 0.2672773\ttotal: 23.6s\tremaining: 20.7s\n",
            "533:\tlearn: 0.2672183\ttotal: 23.6s\tremaining: 20.6s\n",
            "534:\tlearn: 0.2671692\ttotal: 23.7s\tremaining: 20.6s\n",
            "535:\tlearn: 0.2671321\ttotal: 23.7s\tremaining: 20.5s\n",
            "536:\tlearn: 0.2670836\ttotal: 23.8s\tremaining: 20.5s\n",
            "537:\tlearn: 0.2670264\ttotal: 23.8s\tremaining: 20.5s\n",
            "538:\tlearn: 0.2669710\ttotal: 23.9s\tremaining: 20.4s\n",
            "539:\tlearn: 0.2669139\ttotal: 23.9s\tremaining: 20.4s\n",
            "540:\tlearn: 0.2668583\ttotal: 24s\tremaining: 20.3s\n",
            "541:\tlearn: 0.2668014\ttotal: 24s\tremaining: 20.3s\n",
            "542:\tlearn: 0.2667296\ttotal: 24s\tremaining: 20.2s\n",
            "543:\tlearn: 0.2666738\ttotal: 24.1s\tremaining: 20.2s\n",
            "544:\tlearn: 0.2666319\ttotal: 24.1s\tremaining: 20.1s\n",
            "545:\tlearn: 0.2665807\ttotal: 24.2s\tremaining: 20.1s\n",
            "546:\tlearn: 0.2665399\ttotal: 24.2s\tremaining: 20.1s\n",
            "547:\tlearn: 0.2664880\ttotal: 24.3s\tremaining: 20s\n",
            "548:\tlearn: 0.2664823\ttotal: 24.3s\tremaining: 19.9s\n",
            "549:\tlearn: 0.2663931\ttotal: 24.3s\tremaining: 19.9s\n",
            "550:\tlearn: 0.2663289\ttotal: 24.4s\tremaining: 19.9s\n",
            "551:\tlearn: 0.2662843\ttotal: 24.4s\tremaining: 19.8s\n",
            "552:\tlearn: 0.2661858\ttotal: 24.5s\tremaining: 19.8s\n",
            "553:\tlearn: 0.2661097\ttotal: 24.5s\tremaining: 19.7s\n",
            "554:\tlearn: 0.2660471\ttotal: 24.6s\tremaining: 19.7s\n",
            "555:\tlearn: 0.2660427\ttotal: 24.6s\tremaining: 19.6s\n",
            "556:\tlearn: 0.2659543\ttotal: 24.6s\tremaining: 19.6s\n",
            "557:\tlearn: 0.2659092\ttotal: 24.7s\tremaining: 19.5s\n",
            "558:\tlearn: 0.2658722\ttotal: 24.7s\tremaining: 19.5s\n",
            "559:\tlearn: 0.2658238\ttotal: 24.7s\tremaining: 19.4s\n",
            "560:\tlearn: 0.2657572\ttotal: 24.8s\tremaining: 19.4s\n",
            "561:\tlearn: 0.2657002\ttotal: 24.8s\tremaining: 19.4s\n",
            "562:\tlearn: 0.2656612\ttotal: 24.9s\tremaining: 19.3s\n",
            "563:\tlearn: 0.2655871\ttotal: 24.9s\tremaining: 19.3s\n",
            "564:\tlearn: 0.2655105\ttotal: 25s\tremaining: 19.2s\n",
            "565:\tlearn: 0.2654491\ttotal: 25s\tremaining: 19.2s\n",
            "566:\tlearn: 0.2653711\ttotal: 25.1s\tremaining: 19.1s\n",
            "567:\tlearn: 0.2653209\ttotal: 25.1s\tremaining: 19.1s\n",
            "568:\tlearn: 0.2652643\ttotal: 25.2s\tremaining: 19.1s\n",
            "569:\tlearn: 0.2652039\ttotal: 25.2s\tremaining: 19s\n",
            "570:\tlearn: 0.2651630\ttotal: 25.3s\tremaining: 19s\n",
            "571:\tlearn: 0.2651003\ttotal: 25.3s\tremaining: 18.9s\n",
            "572:\tlearn: 0.2650159\ttotal: 25.3s\tremaining: 18.9s\n",
            "573:\tlearn: 0.2649635\ttotal: 25.4s\tremaining: 18.8s\n",
            "574:\tlearn: 0.2649236\ttotal: 25.4s\tremaining: 18.8s\n",
            "575:\tlearn: 0.2648810\ttotal: 25.5s\tremaining: 18.8s\n",
            "576:\tlearn: 0.2648349\ttotal: 25.5s\tremaining: 18.7s\n",
            "577:\tlearn: 0.2647896\ttotal: 25.6s\tremaining: 18.7s\n",
            "578:\tlearn: 0.2647505\ttotal: 25.6s\tremaining: 18.6s\n",
            "579:\tlearn: 0.2646750\ttotal: 25.7s\tremaining: 18.6s\n",
            "580:\tlearn: 0.2646232\ttotal: 25.7s\tremaining: 18.6s\n",
            "581:\tlearn: 0.2645562\ttotal: 25.8s\tremaining: 18.5s\n",
            "582:\tlearn: 0.2644588\ttotal: 25.8s\tremaining: 18.5s\n",
            "583:\tlearn: 0.2643925\ttotal: 25.9s\tremaining: 18.4s\n",
            "584:\tlearn: 0.2643588\ttotal: 25.9s\tremaining: 18.4s\n",
            "585:\tlearn: 0.2643036\ttotal: 26s\tremaining: 18.3s\n",
            "586:\tlearn: 0.2642456\ttotal: 26s\tremaining: 18.3s\n",
            "587:\tlearn: 0.2641715\ttotal: 26s\tremaining: 18.3s\n",
            "588:\tlearn: 0.2641233\ttotal: 26.1s\tremaining: 18.2s\n",
            "589:\tlearn: 0.2640383\ttotal: 26.1s\tremaining: 18.2s\n",
            "590:\tlearn: 0.2639745\ttotal: 26.2s\tremaining: 18.1s\n",
            "591:\tlearn: 0.2638767\ttotal: 26.2s\tremaining: 18.1s\n",
            "592:\tlearn: 0.2638185\ttotal: 26.3s\tremaining: 18s\n",
            "593:\tlearn: 0.2637573\ttotal: 26.3s\tremaining: 18s\n",
            "594:\tlearn: 0.2637152\ttotal: 26.4s\tremaining: 18s\n",
            "595:\tlearn: 0.2636699\ttotal: 26.4s\tremaining: 17.9s\n",
            "596:\tlearn: 0.2635804\ttotal: 26.5s\tremaining: 17.9s\n",
            "597:\tlearn: 0.2635659\ttotal: 26.5s\tremaining: 17.8s\n",
            "598:\tlearn: 0.2635179\ttotal: 26.6s\tremaining: 17.8s\n",
            "599:\tlearn: 0.2634642\ttotal: 26.6s\tremaining: 17.7s\n",
            "600:\tlearn: 0.2634257\ttotal: 26.6s\tremaining: 17.7s\n",
            "601:\tlearn: 0.2633689\ttotal: 26.7s\tremaining: 17.7s\n",
            "602:\tlearn: 0.2632942\ttotal: 26.7s\tremaining: 17.6s\n",
            "603:\tlearn: 0.2632345\ttotal: 26.8s\tremaining: 17.6s\n",
            "604:\tlearn: 0.2631478\ttotal: 26.8s\tremaining: 17.5s\n",
            "605:\tlearn: 0.2630823\ttotal: 26.9s\tremaining: 17.5s\n",
            "606:\tlearn: 0.2630535\ttotal: 26.9s\tremaining: 17.4s\n",
            "607:\tlearn: 0.2630031\ttotal: 27s\tremaining: 17.4s\n",
            "608:\tlearn: 0.2629621\ttotal: 27s\tremaining: 17.3s\n",
            "609:\tlearn: 0.2629283\ttotal: 27.1s\tremaining: 17.3s\n",
            "610:\tlearn: 0.2628428\ttotal: 27.1s\tremaining: 17.3s\n",
            "611:\tlearn: 0.2627982\ttotal: 27.2s\tremaining: 17.2s\n",
            "612:\tlearn: 0.2627521\ttotal: 27.2s\tremaining: 17.2s\n",
            "613:\tlearn: 0.2627211\ttotal: 27.2s\tremaining: 17.1s\n",
            "614:\tlearn: 0.2626542\ttotal: 27.3s\tremaining: 17.1s\n",
            "615:\tlearn: 0.2626269\ttotal: 27.3s\tremaining: 17s\n",
            "616:\tlearn: 0.2625397\ttotal: 27.4s\tremaining: 17s\n",
            "617:\tlearn: 0.2624867\ttotal: 27.4s\tremaining: 16.9s\n",
            "618:\tlearn: 0.2624428\ttotal: 27.5s\tremaining: 16.9s\n",
            "619:\tlearn: 0.2623632\ttotal: 27.5s\tremaining: 16.9s\n",
            "620:\tlearn: 0.2622747\ttotal: 27.6s\tremaining: 16.8s\n",
            "621:\tlearn: 0.2622141\ttotal: 27.6s\tremaining: 16.8s\n",
            "622:\tlearn: 0.2621415\ttotal: 27.6s\tremaining: 16.7s\n",
            "623:\tlearn: 0.2620855\ttotal: 27.7s\tremaining: 16.7s\n",
            "624:\tlearn: 0.2620334\ttotal: 27.7s\tremaining: 16.6s\n",
            "625:\tlearn: 0.2619757\ttotal: 27.8s\tremaining: 16.6s\n",
            "626:\tlearn: 0.2619263\ttotal: 27.8s\tremaining: 16.6s\n",
            "627:\tlearn: 0.2618739\ttotal: 27.9s\tremaining: 16.5s\n",
            "628:\tlearn: 0.2618292\ttotal: 27.9s\tremaining: 16.5s\n",
            "629:\tlearn: 0.2617816\ttotal: 28s\tremaining: 16.4s\n",
            "630:\tlearn: 0.2616922\ttotal: 28s\tremaining: 16.4s\n",
            "631:\tlearn: 0.2616440\ttotal: 28.1s\tremaining: 16.3s\n",
            "632:\tlearn: 0.2615907\ttotal: 28.1s\tremaining: 16.3s\n",
            "633:\tlearn: 0.2615292\ttotal: 28.1s\tremaining: 16.2s\n",
            "634:\tlearn: 0.2614716\ttotal: 28.2s\tremaining: 16.2s\n",
            "635:\tlearn: 0.2614075\ttotal: 28.2s\tremaining: 16.2s\n",
            "636:\tlearn: 0.2613604\ttotal: 28.3s\tremaining: 16.1s\n",
            "637:\tlearn: 0.2612913\ttotal: 28.3s\tremaining: 16.1s\n",
            "638:\tlearn: 0.2612605\ttotal: 28.4s\tremaining: 16s\n",
            "639:\tlearn: 0.2611851\ttotal: 28.4s\tremaining: 16s\n",
            "640:\tlearn: 0.2611168\ttotal: 28.5s\tremaining: 15.9s\n",
            "641:\tlearn: 0.2610500\ttotal: 28.5s\tremaining: 15.9s\n",
            "642:\tlearn: 0.2609793\ttotal: 28.5s\tremaining: 15.8s\n",
            "643:\tlearn: 0.2609382\ttotal: 28.6s\tremaining: 15.8s\n",
            "644:\tlearn: 0.2608790\ttotal: 28.6s\tremaining: 15.8s\n",
            "645:\tlearn: 0.2608427\ttotal: 28.7s\tremaining: 15.7s\n",
            "646:\tlearn: 0.2607435\ttotal: 28.7s\tremaining: 15.7s\n",
            "647:\tlearn: 0.2607053\ttotal: 28.8s\tremaining: 15.6s\n",
            "648:\tlearn: 0.2606544\ttotal: 28.8s\tremaining: 15.6s\n",
            "649:\tlearn: 0.2606043\ttotal: 28.9s\tremaining: 15.5s\n",
            "650:\tlearn: 0.2605540\ttotal: 28.9s\tremaining: 15.5s\n",
            "651:\tlearn: 0.2605109\ttotal: 29s\tremaining: 15.5s\n",
            "652:\tlearn: 0.2604701\ttotal: 29s\tremaining: 15.4s\n",
            "653:\tlearn: 0.2604267\ttotal: 29s\tremaining: 15.4s\n",
            "654:\tlearn: 0.2603846\ttotal: 29.1s\tremaining: 15.3s\n",
            "655:\tlearn: 0.2603311\ttotal: 29.1s\tremaining: 15.3s\n",
            "656:\tlearn: 0.2602771\ttotal: 29.2s\tremaining: 15.2s\n",
            "657:\tlearn: 0.2602312\ttotal: 29.2s\tremaining: 15.2s\n",
            "658:\tlearn: 0.2601770\ttotal: 29.3s\tremaining: 15.2s\n",
            "659:\tlearn: 0.2601436\ttotal: 29.3s\tremaining: 15.1s\n",
            "660:\tlearn: 0.2601000\ttotal: 29.4s\tremaining: 15.1s\n",
            "661:\tlearn: 0.2600297\ttotal: 29.4s\tremaining: 15s\n",
            "662:\tlearn: 0.2599828\ttotal: 29.5s\tremaining: 15s\n",
            "663:\tlearn: 0.2599416\ttotal: 29.5s\tremaining: 14.9s\n",
            "664:\tlearn: 0.2598776\ttotal: 29.6s\tremaining: 14.9s\n",
            "665:\tlearn: 0.2598187\ttotal: 29.6s\tremaining: 14.8s\n",
            "666:\tlearn: 0.2597603\ttotal: 29.6s\tremaining: 14.8s\n",
            "667:\tlearn: 0.2597158\ttotal: 29.7s\tremaining: 14.8s\n",
            "668:\tlearn: 0.2596708\ttotal: 29.7s\tremaining: 14.7s\n",
            "669:\tlearn: 0.2596322\ttotal: 29.8s\tremaining: 14.7s\n",
            "670:\tlearn: 0.2595986\ttotal: 29.8s\tremaining: 14.6s\n",
            "671:\tlearn: 0.2595332\ttotal: 29.9s\tremaining: 14.6s\n",
            "672:\tlearn: 0.2594978\ttotal: 29.9s\tremaining: 14.5s\n",
            "673:\tlearn: 0.2594270\ttotal: 30s\tremaining: 14.5s\n",
            "674:\tlearn: 0.2593981\ttotal: 30s\tremaining: 14.5s\n",
            "675:\tlearn: 0.2593423\ttotal: 30.1s\tremaining: 14.4s\n",
            "676:\tlearn: 0.2592789\ttotal: 30.1s\tremaining: 14.4s\n",
            "677:\tlearn: 0.2592611\ttotal: 30.2s\tremaining: 14.3s\n",
            "678:\tlearn: 0.2592114\ttotal: 30.2s\tremaining: 14.3s\n",
            "679:\tlearn: 0.2591420\ttotal: 30.2s\tremaining: 14.2s\n",
            "680:\tlearn: 0.2590813\ttotal: 30.3s\tremaining: 14.2s\n",
            "681:\tlearn: 0.2590315\ttotal: 30.3s\tremaining: 14.1s\n",
            "682:\tlearn: 0.2589895\ttotal: 30.4s\tremaining: 14.1s\n",
            "683:\tlearn: 0.2589049\ttotal: 30.4s\tremaining: 14.1s\n",
            "684:\tlearn: 0.2588723\ttotal: 30.5s\tremaining: 14s\n",
            "685:\tlearn: 0.2588133\ttotal: 30.5s\tremaining: 14s\n",
            "686:\tlearn: 0.2587566\ttotal: 30.6s\tremaining: 13.9s\n",
            "687:\tlearn: 0.2587260\ttotal: 30.6s\tremaining: 13.9s\n",
            "688:\tlearn: 0.2586782\ttotal: 30.7s\tremaining: 13.8s\n",
            "689:\tlearn: 0.2586223\ttotal: 30.7s\tremaining: 13.8s\n",
            "690:\tlearn: 0.2585468\ttotal: 30.7s\tremaining: 13.8s\n",
            "691:\tlearn: 0.2584778\ttotal: 30.8s\tremaining: 13.7s\n",
            "692:\tlearn: 0.2584263\ttotal: 30.8s\tremaining: 13.7s\n",
            "693:\tlearn: 0.2583894\ttotal: 30.9s\tremaining: 13.6s\n",
            "694:\tlearn: 0.2583378\ttotal: 30.9s\tremaining: 13.6s\n",
            "695:\tlearn: 0.2582963\ttotal: 31s\tremaining: 13.5s\n",
            "696:\tlearn: 0.2582403\ttotal: 31s\tremaining: 13.5s\n",
            "697:\tlearn: 0.2581887\ttotal: 31.1s\tremaining: 13.4s\n",
            "698:\tlearn: 0.2581333\ttotal: 31.1s\tremaining: 13.4s\n",
            "699:\tlearn: 0.2580826\ttotal: 31.2s\tremaining: 13.4s\n",
            "700:\tlearn: 0.2580321\ttotal: 31.2s\tremaining: 13.3s\n",
            "701:\tlearn: 0.2579806\ttotal: 31.3s\tremaining: 13.3s\n",
            "702:\tlearn: 0.2579233\ttotal: 31.3s\tremaining: 13.2s\n",
            "703:\tlearn: 0.2578647\ttotal: 31.4s\tremaining: 13.2s\n",
            "704:\tlearn: 0.2578005\ttotal: 31.4s\tremaining: 13.1s\n",
            "705:\tlearn: 0.2577490\ttotal: 31.4s\tremaining: 13.1s\n",
            "706:\tlearn: 0.2576896\ttotal: 31.5s\tremaining: 13.1s\n",
            "707:\tlearn: 0.2576177\ttotal: 31.5s\tremaining: 13s\n",
            "708:\tlearn: 0.2575443\ttotal: 31.6s\tremaining: 13s\n",
            "709:\tlearn: 0.2574938\ttotal: 31.6s\tremaining: 12.9s\n",
            "710:\tlearn: 0.2574518\ttotal: 31.7s\tremaining: 12.9s\n",
            "711:\tlearn: 0.2574108\ttotal: 31.7s\tremaining: 12.8s\n",
            "712:\tlearn: 0.2573683\ttotal: 31.8s\tremaining: 12.8s\n",
            "713:\tlearn: 0.2573274\ttotal: 31.8s\tremaining: 12.8s\n",
            "714:\tlearn: 0.2573148\ttotal: 31.9s\tremaining: 12.7s\n",
            "715:\tlearn: 0.2572675\ttotal: 31.9s\tremaining: 12.7s\n",
            "716:\tlearn: 0.2572062\ttotal: 31.9s\tremaining: 12.6s\n",
            "717:\tlearn: 0.2571591\ttotal: 32s\tremaining: 12.6s\n",
            "718:\tlearn: 0.2571226\ttotal: 32s\tremaining: 12.5s\n",
            "719:\tlearn: 0.2570526\ttotal: 32.1s\tremaining: 12.5s\n",
            "720:\tlearn: 0.2570088\ttotal: 32.1s\tremaining: 12.4s\n",
            "721:\tlearn: 0.2569928\ttotal: 32.2s\tremaining: 12.4s\n",
            "722:\tlearn: 0.2569317\ttotal: 32.2s\tremaining: 12.3s\n",
            "723:\tlearn: 0.2568696\ttotal: 32.3s\tremaining: 12.3s\n",
            "724:\tlearn: 0.2568199\ttotal: 32.3s\tremaining: 12.3s\n",
            "725:\tlearn: 0.2567754\ttotal: 32.4s\tremaining: 12.2s\n",
            "726:\tlearn: 0.2567311\ttotal: 32.4s\tremaining: 12.2s\n",
            "727:\tlearn: 0.2567029\ttotal: 32.5s\tremaining: 12.1s\n",
            "728:\tlearn: 0.2566585\ttotal: 32.5s\tremaining: 12.1s\n",
            "729:\tlearn: 0.2566204\ttotal: 32.6s\tremaining: 12s\n",
            "730:\tlearn: 0.2565559\ttotal: 32.6s\tremaining: 12s\n",
            "731:\tlearn: 0.2565256\ttotal: 32.7s\tremaining: 12s\n",
            "732:\tlearn: 0.2564819\ttotal: 32.7s\tremaining: 11.9s\n",
            "733:\tlearn: 0.2564432\ttotal: 32.8s\tremaining: 11.9s\n",
            "734:\tlearn: 0.2563956\ttotal: 32.8s\tremaining: 11.8s\n",
            "735:\tlearn: 0.2563512\ttotal: 32.8s\tremaining: 11.8s\n",
            "736:\tlearn: 0.2563136\ttotal: 32.9s\tremaining: 11.7s\n",
            "737:\tlearn: 0.2562590\ttotal: 32.9s\tremaining: 11.7s\n",
            "738:\tlearn: 0.2562312\ttotal: 33s\tremaining: 11.7s\n",
            "739:\tlearn: 0.2562263\ttotal: 33s\tremaining: 11.6s\n",
            "740:\tlearn: 0.2561606\ttotal: 33.1s\tremaining: 11.6s\n",
            "741:\tlearn: 0.2561128\ttotal: 33.1s\tremaining: 11.5s\n",
            "742:\tlearn: 0.2560664\ttotal: 33.1s\tremaining: 11.5s\n",
            "743:\tlearn: 0.2560234\ttotal: 33.2s\tremaining: 11.4s\n",
            "744:\tlearn: 0.2559844\ttotal: 33.2s\tremaining: 11.4s\n",
            "745:\tlearn: 0.2559328\ttotal: 33.3s\tremaining: 11.3s\n",
            "746:\tlearn: 0.2559107\ttotal: 33.3s\tremaining: 11.3s\n",
            "747:\tlearn: 0.2558568\ttotal: 33.4s\tremaining: 11.2s\n",
            "748:\tlearn: 0.2558290\ttotal: 33.4s\tremaining: 11.2s\n",
            "749:\tlearn: 0.2557926\ttotal: 33.5s\tremaining: 11.2s\n",
            "750:\tlearn: 0.2557887\ttotal: 33.5s\tremaining: 11.1s\n",
            "751:\tlearn: 0.2557369\ttotal: 33.5s\tremaining: 11.1s\n",
            "752:\tlearn: 0.2556727\ttotal: 33.6s\tremaining: 11s\n",
            "753:\tlearn: 0.2556304\ttotal: 33.6s\tremaining: 11s\n",
            "754:\tlearn: 0.2555997\ttotal: 33.7s\tremaining: 10.9s\n",
            "755:\tlearn: 0.2555668\ttotal: 33.7s\tremaining: 10.9s\n",
            "756:\tlearn: 0.2555265\ttotal: 33.8s\tremaining: 10.8s\n",
            "757:\tlearn: 0.2554746\ttotal: 33.8s\tremaining: 10.8s\n",
            "758:\tlearn: 0.2554325\ttotal: 33.9s\tremaining: 10.8s\n",
            "759:\tlearn: 0.2554040\ttotal: 33.9s\tremaining: 10.7s\n",
            "760:\tlearn: 0.2553498\ttotal: 34s\tremaining: 10.7s\n",
            "761:\tlearn: 0.2552776\ttotal: 34s\tremaining: 10.6s\n",
            "762:\tlearn: 0.2552333\ttotal: 34.1s\tremaining: 10.6s\n",
            "763:\tlearn: 0.2551462\ttotal: 34.1s\tremaining: 10.5s\n",
            "764:\tlearn: 0.2550726\ttotal: 34.2s\tremaining: 10.5s\n",
            "765:\tlearn: 0.2550476\ttotal: 34.2s\tremaining: 10.4s\n",
            "766:\tlearn: 0.2549666\ttotal: 34.3s\tremaining: 10.4s\n",
            "767:\tlearn: 0.2548915\ttotal: 34.3s\tremaining: 10.4s\n",
            "768:\tlearn: 0.2548321\ttotal: 34.4s\tremaining: 10.3s\n",
            "769:\tlearn: 0.2548076\ttotal: 34.4s\tremaining: 10.3s\n",
            "770:\tlearn: 0.2547813\ttotal: 34.4s\tremaining: 10.2s\n",
            "771:\tlearn: 0.2547380\ttotal: 34.5s\tremaining: 10.2s\n",
            "772:\tlearn: 0.2546700\ttotal: 34.5s\tremaining: 10.1s\n",
            "773:\tlearn: 0.2546290\ttotal: 34.6s\tremaining: 10.1s\n",
            "774:\tlearn: 0.2545760\ttotal: 34.6s\tremaining: 10.1s\n",
            "775:\tlearn: 0.2545446\ttotal: 34.7s\tremaining: 10s\n",
            "776:\tlearn: 0.2545013\ttotal: 34.7s\tremaining: 9.97s\n",
            "777:\tlearn: 0.2544677\ttotal: 34.8s\tremaining: 9.93s\n",
            "778:\tlearn: 0.2544098\ttotal: 34.8s\tremaining: 9.88s\n",
            "779:\tlearn: 0.2543525\ttotal: 34.9s\tremaining: 9.84s\n",
            "780:\tlearn: 0.2542914\ttotal: 34.9s\tremaining: 9.79s\n",
            "781:\tlearn: 0.2542255\ttotal: 35s\tremaining: 9.75s\n",
            "782:\tlearn: 0.2541694\ttotal: 35s\tremaining: 9.71s\n",
            "783:\tlearn: 0.2541097\ttotal: 35.1s\tremaining: 9.66s\n",
            "784:\tlearn: 0.2540634\ttotal: 35.1s\tremaining: 9.62s\n",
            "785:\tlearn: 0.2540197\ttotal: 35.2s\tremaining: 9.57s\n",
            "786:\tlearn: 0.2539866\ttotal: 35.2s\tremaining: 9.53s\n",
            "787:\tlearn: 0.2539247\ttotal: 35.3s\tremaining: 9.49s\n",
            "788:\tlearn: 0.2538761\ttotal: 35.3s\tremaining: 9.44s\n",
            "789:\tlearn: 0.2538381\ttotal: 35.4s\tremaining: 9.4s\n",
            "790:\tlearn: 0.2537877\ttotal: 35.4s\tremaining: 9.35s\n",
            "791:\tlearn: 0.2537154\ttotal: 35.4s\tremaining: 9.31s\n",
            "792:\tlearn: 0.2537065\ttotal: 35.5s\tremaining: 9.26s\n",
            "793:\tlearn: 0.2536619\ttotal: 35.5s\tremaining: 9.21s\n",
            "794:\tlearn: 0.2536339\ttotal: 35.6s\tremaining: 9.17s\n",
            "795:\tlearn: 0.2535895\ttotal: 35.6s\tremaining: 9.12s\n",
            "796:\tlearn: 0.2535506\ttotal: 35.6s\tremaining: 9.08s\n",
            "797:\tlearn: 0.2534865\ttotal: 35.7s\tremaining: 9.03s\n",
            "798:\tlearn: 0.2534299\ttotal: 35.7s\tremaining: 8.99s\n",
            "799:\tlearn: 0.2533688\ttotal: 35.8s\tremaining: 8.95s\n",
            "800:\tlearn: 0.2533446\ttotal: 35.8s\tremaining: 8.9s\n",
            "801:\tlearn: 0.2533047\ttotal: 35.9s\tremaining: 8.86s\n",
            "802:\tlearn: 0.2532608\ttotal: 35.9s\tremaining: 8.81s\n",
            "803:\tlearn: 0.2532162\ttotal: 36s\tremaining: 8.77s\n",
            "804:\tlearn: 0.2531697\ttotal: 36s\tremaining: 8.72s\n",
            "805:\tlearn: 0.2531258\ttotal: 36.1s\tremaining: 8.68s\n",
            "806:\tlearn: 0.2530927\ttotal: 36.1s\tremaining: 8.63s\n",
            "807:\tlearn: 0.2530657\ttotal: 36.1s\tremaining: 8.59s\n",
            "808:\tlearn: 0.2530126\ttotal: 36.2s\tremaining: 8.54s\n",
            "809:\tlearn: 0.2529805\ttotal: 36.2s\tremaining: 8.5s\n",
            "810:\tlearn: 0.2529018\ttotal: 36.3s\tremaining: 8.45s\n",
            "811:\tlearn: 0.2528410\ttotal: 36.3s\tremaining: 8.41s\n",
            "812:\tlearn: 0.2527881\ttotal: 36.4s\tremaining: 8.37s\n",
            "813:\tlearn: 0.2527374\ttotal: 36.4s\tremaining: 8.32s\n",
            "814:\tlearn: 0.2527085\ttotal: 36.5s\tremaining: 8.28s\n",
            "815:\tlearn: 0.2526121\ttotal: 36.5s\tremaining: 8.23s\n",
            "816:\tlearn: 0.2525686\ttotal: 36.5s\tremaining: 8.19s\n",
            "817:\tlearn: 0.2525302\ttotal: 36.6s\tremaining: 8.14s\n",
            "818:\tlearn: 0.2524920\ttotal: 36.6s\tremaining: 8.1s\n",
            "819:\tlearn: 0.2524688\ttotal: 36.7s\tremaining: 8.05s\n",
            "820:\tlearn: 0.2523909\ttotal: 36.7s\tremaining: 8.01s\n",
            "821:\tlearn: 0.2523335\ttotal: 36.8s\tremaining: 7.96s\n",
            "822:\tlearn: 0.2522721\ttotal: 36.8s\tremaining: 7.92s\n",
            "823:\tlearn: 0.2522412\ttotal: 36.9s\tremaining: 7.88s\n",
            "824:\tlearn: 0.2521874\ttotal: 36.9s\tremaining: 7.83s\n",
            "825:\tlearn: 0.2521420\ttotal: 37s\tremaining: 7.79s\n",
            "826:\tlearn: 0.2520867\ttotal: 37s\tremaining: 7.74s\n",
            "827:\tlearn: 0.2520257\ttotal: 37.1s\tremaining: 7.7s\n",
            "828:\tlearn: 0.2519683\ttotal: 37.1s\tremaining: 7.65s\n",
            "829:\tlearn: 0.2518711\ttotal: 37.1s\tremaining: 7.61s\n",
            "830:\tlearn: 0.2518296\ttotal: 37.2s\tremaining: 7.56s\n",
            "831:\tlearn: 0.2517716\ttotal: 37.2s\tremaining: 7.52s\n",
            "832:\tlearn: 0.2517202\ttotal: 37.3s\tremaining: 7.47s\n",
            "833:\tlearn: 0.2516833\ttotal: 37.3s\tremaining: 7.43s\n",
            "834:\tlearn: 0.2516233\ttotal: 37.4s\tremaining: 7.38s\n",
            "835:\tlearn: 0.2515754\ttotal: 37.4s\tremaining: 7.34s\n",
            "836:\tlearn: 0.2515283\ttotal: 37.5s\tremaining: 7.29s\n",
            "837:\tlearn: 0.2515066\ttotal: 37.5s\tremaining: 7.25s\n",
            "838:\tlearn: 0.2514491\ttotal: 37.6s\tremaining: 7.21s\n",
            "839:\tlearn: 0.2514064\ttotal: 37.6s\tremaining: 7.16s\n",
            "840:\tlearn: 0.2513763\ttotal: 37.6s\tremaining: 7.12s\n",
            "841:\tlearn: 0.2513123\ttotal: 37.7s\tremaining: 7.07s\n",
            "842:\tlearn: 0.2512666\ttotal: 37.7s\tremaining: 7.03s\n",
            "843:\tlearn: 0.2512267\ttotal: 37.8s\tremaining: 6.98s\n",
            "844:\tlearn: 0.2511909\ttotal: 37.8s\tremaining: 6.94s\n",
            "845:\tlearn: 0.2511504\ttotal: 37.9s\tremaining: 6.89s\n",
            "846:\tlearn: 0.2510892\ttotal: 37.9s\tremaining: 6.85s\n",
            "847:\tlearn: 0.2510521\ttotal: 38s\tremaining: 6.8s\n",
            "848:\tlearn: 0.2510111\ttotal: 38s\tremaining: 6.76s\n",
            "849:\tlearn: 0.2509496\ttotal: 38s\tremaining: 6.71s\n",
            "850:\tlearn: 0.2509258\ttotal: 38.1s\tremaining: 6.67s\n",
            "851:\tlearn: 0.2509029\ttotal: 38.1s\tremaining: 6.62s\n",
            "852:\tlearn: 0.2508654\ttotal: 38.2s\tremaining: 6.58s\n",
            "853:\tlearn: 0.2508010\ttotal: 38.2s\tremaining: 6.53s\n",
            "854:\tlearn: 0.2507576\ttotal: 38.3s\tremaining: 6.49s\n",
            "855:\tlearn: 0.2507176\ttotal: 38.3s\tremaining: 6.44s\n",
            "856:\tlearn: 0.2506848\ttotal: 38.3s\tremaining: 6.4s\n",
            "857:\tlearn: 0.2506485\ttotal: 38.4s\tremaining: 6.35s\n",
            "858:\tlearn: 0.2506153\ttotal: 38.4s\tremaining: 6.31s\n",
            "859:\tlearn: 0.2505747\ttotal: 38.5s\tremaining: 6.26s\n",
            "860:\tlearn: 0.2505211\ttotal: 38.5s\tremaining: 6.22s\n",
            "861:\tlearn: 0.2504972\ttotal: 38.6s\tremaining: 6.17s\n",
            "862:\tlearn: 0.2504377\ttotal: 38.6s\tremaining: 6.13s\n",
            "863:\tlearn: 0.2504026\ttotal: 38.7s\tremaining: 6.09s\n",
            "864:\tlearn: 0.2503259\ttotal: 38.7s\tremaining: 6.04s\n",
            "865:\tlearn: 0.2503035\ttotal: 38.8s\tremaining: 6s\n",
            "866:\tlearn: 0.2502493\ttotal: 38.8s\tremaining: 5.95s\n",
            "867:\tlearn: 0.2502129\ttotal: 38.8s\tremaining: 5.9s\n",
            "868:\tlearn: 0.2501807\ttotal: 38.9s\tremaining: 5.86s\n",
            "869:\tlearn: 0.2501379\ttotal: 38.9s\tremaining: 5.82s\n",
            "870:\tlearn: 0.2501054\ttotal: 39s\tremaining: 5.77s\n",
            "871:\tlearn: 0.2500435\ttotal: 39s\tremaining: 5.72s\n",
            "872:\tlearn: 0.2499657\ttotal: 39.1s\tremaining: 5.68s\n",
            "873:\tlearn: 0.2499303\ttotal: 39.1s\tremaining: 5.64s\n",
            "874:\tlearn: 0.2498765\ttotal: 39.2s\tremaining: 5.59s\n",
            "875:\tlearn: 0.2498400\ttotal: 39.2s\tremaining: 5.55s\n",
            "876:\tlearn: 0.2497843\ttotal: 39.2s\tremaining: 5.5s\n",
            "877:\tlearn: 0.2497407\ttotal: 39.3s\tremaining: 5.46s\n",
            "878:\tlearn: 0.2497084\ttotal: 39.3s\tremaining: 5.42s\n",
            "879:\tlearn: 0.2496721\ttotal: 39.4s\tremaining: 5.37s\n",
            "880:\tlearn: 0.2496449\ttotal: 39.4s\tremaining: 5.33s\n",
            "881:\tlearn: 0.2495818\ttotal: 39.5s\tremaining: 5.28s\n",
            "882:\tlearn: 0.2495446\ttotal: 39.5s\tremaining: 5.24s\n",
            "883:\tlearn: 0.2495012\ttotal: 39.6s\tremaining: 5.19s\n",
            "884:\tlearn: 0.2494728\ttotal: 39.6s\tremaining: 5.15s\n",
            "885:\tlearn: 0.2494465\ttotal: 39.7s\tremaining: 5.1s\n",
            "886:\tlearn: 0.2493962\ttotal: 39.7s\tremaining: 5.06s\n",
            "887:\tlearn: 0.2493547\ttotal: 39.7s\tremaining: 5.01s\n",
            "888:\tlearn: 0.2492978\ttotal: 39.8s\tremaining: 4.97s\n",
            "889:\tlearn: 0.2492565\ttotal: 39.8s\tremaining: 4.92s\n",
            "890:\tlearn: 0.2491916\ttotal: 39.9s\tremaining: 4.88s\n",
            "891:\tlearn: 0.2491553\ttotal: 39.9s\tremaining: 4.83s\n",
            "892:\tlearn: 0.2490818\ttotal: 40s\tremaining: 4.79s\n",
            "893:\tlearn: 0.2490422\ttotal: 40s\tremaining: 4.74s\n",
            "894:\tlearn: 0.2489732\ttotal: 40.1s\tremaining: 4.7s\n",
            "895:\tlearn: 0.2489273\ttotal: 40.1s\tremaining: 4.66s\n",
            "896:\tlearn: 0.2488747\ttotal: 40.2s\tremaining: 4.61s\n",
            "897:\tlearn: 0.2488371\ttotal: 40.2s\tremaining: 4.57s\n",
            "898:\tlearn: 0.2487808\ttotal: 40.2s\tremaining: 4.52s\n",
            "899:\tlearn: 0.2487421\ttotal: 40.3s\tremaining: 4.48s\n",
            "900:\tlearn: 0.2486703\ttotal: 40.3s\tremaining: 4.43s\n",
            "901:\tlearn: 0.2486298\ttotal: 40.4s\tremaining: 4.39s\n",
            "902:\tlearn: 0.2485888\ttotal: 40.4s\tremaining: 4.34s\n",
            "903:\tlearn: 0.2485597\ttotal: 40.5s\tremaining: 4.3s\n",
            "904:\tlearn: 0.2485166\ttotal: 40.5s\tremaining: 4.25s\n",
            "905:\tlearn: 0.2484882\ttotal: 40.6s\tremaining: 4.21s\n",
            "906:\tlearn: 0.2484538\ttotal: 40.6s\tremaining: 4.16s\n",
            "907:\tlearn: 0.2484066\ttotal: 40.6s\tremaining: 4.12s\n",
            "908:\tlearn: 0.2483631\ttotal: 40.7s\tremaining: 4.07s\n",
            "909:\tlearn: 0.2483257\ttotal: 40.7s\tremaining: 4.03s\n",
            "910:\tlearn: 0.2482884\ttotal: 40.8s\tremaining: 3.98s\n",
            "911:\tlearn: 0.2482489\ttotal: 40.8s\tremaining: 3.94s\n",
            "912:\tlearn: 0.2482059\ttotal: 40.9s\tremaining: 3.89s\n",
            "913:\tlearn: 0.2481618\ttotal: 40.9s\tremaining: 3.85s\n",
            "914:\tlearn: 0.2481096\ttotal: 41s\tremaining: 3.81s\n",
            "915:\tlearn: 0.2480431\ttotal: 41s\tremaining: 3.76s\n",
            "916:\tlearn: 0.2480242\ttotal: 41.1s\tremaining: 3.72s\n",
            "917:\tlearn: 0.2480011\ttotal: 41.1s\tremaining: 3.67s\n",
            "918:\tlearn: 0.2479606\ttotal: 41.2s\tremaining: 3.63s\n",
            "919:\tlearn: 0.2478954\ttotal: 41.2s\tremaining: 3.58s\n",
            "920:\tlearn: 0.2478684\ttotal: 41.2s\tremaining: 3.54s\n",
            "921:\tlearn: 0.2478044\ttotal: 41.3s\tremaining: 3.49s\n",
            "922:\tlearn: 0.2477586\ttotal: 41.3s\tremaining: 3.45s\n",
            "923:\tlearn: 0.2477225\ttotal: 41.4s\tremaining: 3.4s\n",
            "924:\tlearn: 0.2476907\ttotal: 41.4s\tremaining: 3.36s\n",
            "925:\tlearn: 0.2476329\ttotal: 41.5s\tremaining: 3.31s\n",
            "926:\tlearn: 0.2475992\ttotal: 41.5s\tremaining: 3.27s\n",
            "927:\tlearn: 0.2475571\ttotal: 41.6s\tremaining: 3.22s\n",
            "928:\tlearn: 0.2475304\ttotal: 41.6s\tremaining: 3.18s\n",
            "929:\tlearn: 0.2474786\ttotal: 41.6s\tremaining: 3.13s\n",
            "930:\tlearn: 0.2474263\ttotal: 41.7s\tremaining: 3.09s\n",
            "931:\tlearn: 0.2473608\ttotal: 41.7s\tremaining: 3.04s\n",
            "932:\tlearn: 0.2473190\ttotal: 41.8s\tremaining: 3s\n",
            "933:\tlearn: 0.2472953\ttotal: 41.8s\tremaining: 2.96s\n",
            "934:\tlearn: 0.2472660\ttotal: 41.9s\tremaining: 2.91s\n",
            "935:\tlearn: 0.2472124\ttotal: 41.9s\tremaining: 2.87s\n",
            "936:\tlearn: 0.2471481\ttotal: 42s\tremaining: 2.82s\n",
            "937:\tlearn: 0.2470987\ttotal: 42s\tremaining: 2.78s\n",
            "938:\tlearn: 0.2470428\ttotal: 42.1s\tremaining: 2.73s\n",
            "939:\tlearn: 0.2469951\ttotal: 42.1s\tremaining: 2.69s\n",
            "940:\tlearn: 0.2469356\ttotal: 42.1s\tremaining: 2.64s\n",
            "941:\tlearn: 0.2468675\ttotal: 42.2s\tremaining: 2.6s\n",
            "942:\tlearn: 0.2468428\ttotal: 42.2s\tremaining: 2.55s\n",
            "943:\tlearn: 0.2467804\ttotal: 42.3s\tremaining: 2.51s\n",
            "944:\tlearn: 0.2467305\ttotal: 42.3s\tremaining: 2.46s\n",
            "945:\tlearn: 0.2467142\ttotal: 42.4s\tremaining: 2.42s\n",
            "946:\tlearn: 0.2466676\ttotal: 42.4s\tremaining: 2.37s\n",
            "947:\tlearn: 0.2466305\ttotal: 42.5s\tremaining: 2.33s\n",
            "948:\tlearn: 0.2465447\ttotal: 42.5s\tremaining: 2.28s\n",
            "949:\tlearn: 0.2465217\ttotal: 42.6s\tremaining: 2.24s\n",
            "950:\tlearn: 0.2464960\ttotal: 42.6s\tremaining: 2.19s\n",
            "951:\tlearn: 0.2464500\ttotal: 42.6s\tremaining: 2.15s\n",
            "952:\tlearn: 0.2463629\ttotal: 42.7s\tremaining: 2.1s\n",
            "953:\tlearn: 0.2463360\ttotal: 42.7s\tremaining: 2.06s\n",
            "954:\tlearn: 0.2462875\ttotal: 42.8s\tremaining: 2.02s\n",
            "955:\tlearn: 0.2462592\ttotal: 42.8s\tremaining: 1.97s\n",
            "956:\tlearn: 0.2461950\ttotal: 42.9s\tremaining: 1.93s\n",
            "957:\tlearn: 0.2461625\ttotal: 42.9s\tremaining: 1.88s\n",
            "958:\tlearn: 0.2460987\ttotal: 43s\tremaining: 1.84s\n",
            "959:\tlearn: 0.2460630\ttotal: 43s\tremaining: 1.79s\n",
            "960:\tlearn: 0.2460302\ttotal: 43s\tremaining: 1.75s\n",
            "961:\tlearn: 0.2459893\ttotal: 43.1s\tremaining: 1.7s\n",
            "962:\tlearn: 0.2459530\ttotal: 43.1s\tremaining: 1.66s\n",
            "963:\tlearn: 0.2458894\ttotal: 43.2s\tremaining: 1.61s\n",
            "964:\tlearn: 0.2458326\ttotal: 43.2s\tremaining: 1.57s\n",
            "965:\tlearn: 0.2457625\ttotal: 43.3s\tremaining: 1.52s\n",
            "966:\tlearn: 0.2457336\ttotal: 43.3s\tremaining: 1.48s\n",
            "967:\tlearn: 0.2457326\ttotal: 43.3s\tremaining: 1.43s\n",
            "968:\tlearn: 0.2456832\ttotal: 43.4s\tremaining: 1.39s\n",
            "969:\tlearn: 0.2456372\ttotal: 43.4s\tremaining: 1.34s\n",
            "970:\tlearn: 0.2455786\ttotal: 43.5s\tremaining: 1.3s\n",
            "971:\tlearn: 0.2455367\ttotal: 43.5s\tremaining: 1.25s\n",
            "972:\tlearn: 0.2454871\ttotal: 43.6s\tremaining: 1.21s\n",
            "973:\tlearn: 0.2454475\ttotal: 43.6s\tremaining: 1.16s\n",
            "974:\tlearn: 0.2454019\ttotal: 43.6s\tremaining: 1.12s\n",
            "975:\tlearn: 0.2453408\ttotal: 43.7s\tremaining: 1.07s\n",
            "976:\tlearn: 0.2453135\ttotal: 43.7s\tremaining: 1.03s\n",
            "977:\tlearn: 0.2452515\ttotal: 43.8s\tremaining: 985ms\n",
            "978:\tlearn: 0.2452057\ttotal: 43.8s\tremaining: 940ms\n",
            "979:\tlearn: 0.2451661\ttotal: 43.9s\tremaining: 895ms\n",
            "980:\tlearn: 0.2451346\ttotal: 43.9s\tremaining: 850ms\n",
            "981:\tlearn: 0.2451107\ttotal: 43.9s\tremaining: 806ms\n",
            "982:\tlearn: 0.2450685\ttotal: 44s\tremaining: 761ms\n",
            "983:\tlearn: 0.2450259\ttotal: 44s\tremaining: 716ms\n",
            "984:\tlearn: 0.2449621\ttotal: 44.1s\tremaining: 671ms\n",
            "985:\tlearn: 0.2449370\ttotal: 44.1s\tremaining: 627ms\n",
            "986:\tlearn: 0.2448674\ttotal: 44.2s\tremaining: 582ms\n",
            "987:\tlearn: 0.2448344\ttotal: 44.2s\tremaining: 537ms\n",
            "988:\tlearn: 0.2447948\ttotal: 44.3s\tremaining: 492ms\n",
            "989:\tlearn: 0.2447450\ttotal: 44.3s\tremaining: 448ms\n",
            "990:\tlearn: 0.2447081\ttotal: 44.4s\tremaining: 403ms\n",
            "991:\tlearn: 0.2446703\ttotal: 44.4s\tremaining: 358ms\n",
            "992:\tlearn: 0.2446341\ttotal: 44.4s\tremaining: 313ms\n",
            "993:\tlearn: 0.2445996\ttotal: 44.5s\tremaining: 269ms\n",
            "994:\tlearn: 0.2445092\ttotal: 44.5s\tremaining: 224ms\n",
            "995:\tlearn: 0.2444349\ttotal: 44.6s\tremaining: 179ms\n",
            "996:\tlearn: 0.2443806\ttotal: 44.6s\tremaining: 134ms\n",
            "997:\tlearn: 0.2443480\ttotal: 44.7s\tremaining: 89.5ms\n",
            "998:\tlearn: 0.2442811\ttotal: 44.7s\tremaining: 44.8ms\n",
            "999:\tlearn: 0.2442524\ttotal: 44.8s\tremaining: 0us\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ncXnd9W4XG1-"
      },
      "source": [
        "###### Matrice de confusion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fV5tkE0bXG1_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "outputId": "b7458609-e597-4b67-a31a-5784d0e8ca3c"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "pd.DataFrame(confusion_matrix(y_train_label, y_pred), columns = target_label.classes_, index = target_label.classes_)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>&lt;=50K</th>\n",
              "      <th>&gt;50K</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>&lt;=50K</th>\n",
              "      <td>23917</td>\n",
              "      <td>1032</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>&gt;50K</th>\n",
              "      <td>2497</td>\n",
              "      <td>5277</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         <=50K   >50K\n",
              " <=50K   23917   1032\n",
              " >50K     2497   5277"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2HyBqtMjXG2B"
      },
      "source": [
        "###### Distribution des classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Z_emYEYKXG2B",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "outputId": "c1c143ed-829d-4148-ec03-ec927ade8562"
      },
      "source": [
        "import matplotlib.pyplot as plt \n",
        "print(target_label.classes_)\n",
        "pd.Series(y_train_label).hist()"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[' <=50K' ' >50K']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f69fc081358>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAD4CAYAAAAO9oqkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nDANJEoaBJAnDQJKEYSBJAv4X42P4cFKtnHoAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ee-L7b131tf3"
      },
      "source": [
        "Justification avec la matrice de confusion et la distribution des classes\n",
        "\n",
        "Avec la matrice de confusion, on voit que les gens ayant un salaire <= 50K sont prédits à 96%, alors que ceux avec un salaire plus élevé que 50K sont prédits à seulement 53%. Cela s'explique probablement par le plus grand nombre de données avec un salaire <= 50K dans le jeu de données d'entraînement. Pour les données avec un salaire plus élevée que 50K, nous avons probablement mal correlés certaines facteurs. Toutefois, avec un score de 0,86066 , on juge avoir tout de même un bon jeu de données modifiées. On ne veut pas faire du overfitting non plus."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5lfReUdgXG2I"
      },
      "source": [
        "## 3.4 Soumission (0.5 point)\n",
        "\n",
        "Enfin, effectuez la prédiction sur l'ensemble de test et joignez les résultats au rendu du TP. Vous devez soumettre vos résultats sur kaggle. Vous pouvez choisir jusqu'à deux modèles. Par défaut, les deux dernières soumissions sont prises. La justification (les section 3.2 et 3.3) sont seulement demandée pour le meilleur modèle selon vous.\n",
        "\n",
        "**ATTENTION:** N'oubliez pas de respecter le format du fichier de soumission. En cas d'erreur, Kaggle score 0.0 pour la soumission."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-deVgNIpXG2J",
        "colab": {}
      },
      "source": [
        "best_model_1 = selected_model\n",
        "\n",
        "pred_test = pd.DataFrame()\n",
        "pred_test['index'] = X_test['index']\n",
        "pred_test['Income'] = pd.Series(best_model_1.predict(X_test_preprocess), dtype='int32')\n",
        "pred_test['Income'] = target_label.inverse_transform(pred_test['Income'])\n",
        "pred_test.to_csv(PATH+\"test_prediction_cat.csv\",index = False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4ky9HZuMXk4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}